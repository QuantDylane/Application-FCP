"""
Valeurs Liquidatives Analysis Page
Analyzes net asset values for FCP funds
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
from datetime import datetime, timedelta
from sklearn.cluster import KMeans
from scipy import stats
import os

# Constants
TRADING_DAYS_PER_YEAR = 252
DATA_FILE = os.getenv('FCP_DATA_FILE', 'data_fcp.xlsx')

# Risk Fingerprint Normalization Constants
# For skewness normalization: transforms skewness values to [0, 100] scale
# Positive skewness (right tail) maps to [50, 100], negative to [0, 50]
SKEWNESS_SCALE_FACTOR = 25  # Scaling factor for skewness transformation
SKEWNESS_NEUTRAL_SCORE = 50  # Score for zero skewness (neutral distribution)

# Color Scheme
PRIMARY_COLOR = "#114B80"    # Bleu profond ‚Äî titres, boutons principaux
SECONDARY_COLOR = "#567389"  # Bleu-gris ‚Äî widgets, lignes, ic√¥nes
ACCENT_COLOR = "#ACC7DF"     # Bleu clair ‚Äî fonds de cartes, hover

def hex_to_rgba(hex_color, alpha=1.0):
    """
    Convert hex color to rgba string format.
    
    Args:
        hex_color (str): Hex color string (e.g., '#114B80' or '114B80')
        alpha (float): Alpha transparency value between 0.0 and 1.0
        
    Returns:
        str: RGBA color string (e.g., 'rgba(17, 75, 128, 0.3)')
    """
    if not 0.0 <= alpha <= 1.0:
        raise ValueError(f"Alpha value must be between 0.0 and 1.0, got {alpha}")
    hex_color = hex_color.lstrip('#')
    if len(hex_color) != 6:
        raise ValueError(f"Invalid hex color format: {hex_color}. Expected 6-character hex string.")
    try:
        r = int(hex_color[0:2], 16)
        g = int(hex_color[2:4], 16)
        b = int(hex_color[4:6], 16)
    except ValueError:
        raise ValueError(f"Invalid hex color format: {hex_color}. Could not parse hex values.")
    return f'rgba({r}, {g}, {b}, {alpha})'

def generate_llm_style_narrative(fcp_name, risk_profile, metrics, strengths, weaknesses):
    """
    Generate an advanced, context-aware narrative analysis using LLM-style logic.
    This function analyzes multiple dimensions and creates a cohesive, professional narrative.
    
    Args:
        fcp_name (str): Name of the FCP
        risk_profile (dict): Normalized risk profile scores
        metrics (dict): Raw metrics (volatility, drawdown, sharpe, etc.)
        strengths (list): Top 3 strengths as (dimension, score) tuples
        weaknesses (list): Bottom 3 weaknesses as (dimension, score) tuples
    
    Returns:
        str: Professional narrative ready for investment committee
    """
    
    # Extract key metrics
    score_global = np.mean(list(risk_profile.values()))
    volatility = metrics.get('volatility', 0)
    max_dd = metrics.get('max_drawdown', 0)
    pain_ratio = metrics.get('pain_ratio', 0)
    skewness = metrics.get('skewness', 0)
    sharpe = metrics.get('sharpe_ratio', 0)
    ulcer = metrics.get('ulcer_index', 0)
    
    # Determine overall quality
    if score_global >= 70:
        quality = "excellent"
        quality_color = "#28a745"
    elif score_global >= 50:
        quality = "satisfaisant"
        quality_color = "#ffc107"
    else:
        quality = "pr√©occupant"
        quality_color = "#dc3545"
    
    # Opening paragraph - contextual introduction
    if score_global >= 70:
        opening = f"Le fonds **{fcp_name}** se distingue par un profil de risque **{quality}** (score global: {score_global:.1f}/100), refl√©tant une gestion rigoureuse et une ma√Ætrise avanc√©e des risques. "
    elif score_global >= 50:
        opening = f"Le fonds **{fcp_name}** pr√©sente un profil de risque **{quality}** (score global: {score_global:.1f}/100), caract√©ris√© par un √©quilibre raisonnable entre potentiel de performance et exposition aux risques. "
    else:
        opening = f"Le fonds **{fcp_name}** affiche un profil de risque **{quality}** (score global: {score_global:.1f}/100), n√©cessitant une vigilance accrue et un suivi rapproch√© des expositions. "
    
    # Volatility analysis with context
    vol_pct = risk_profile.get('volatility', 50)
    if vol_pct >= 70:
        vol_narrative = f"La **volatilit√© remarquablement contenue** ({volatility:.2f}%, score: {vol_pct:.0f}/100) t√©moigne d'une gestion prudente et d'une construction de portefeuille bien diversifi√©e, offrant un confort de d√©tention appr√©ciable pour les investisseurs."
    elif vol_pct >= 40:
        vol_narrative = f"La **volatilit√© mod√©r√©e** ({volatility:.2f}%, score: {vol_pct:.0f}/100) se situe dans une fourchette √©quilibr√©e, permettant de capter des opportunit√©s de march√© tout en limitant les fluctuations excessives."
    else:
        vol_narrative = f"La **volatilit√© √©lev√©e** ({volatility:.2f}%, score: {vol_pct:.0f}/100) refl√®te une exposition significative aux fluctuations de march√©, requ√©rant une tol√©rance au risque importante et un horizon d'investissement appropri√©."
    
    # Drawdown and resilience analysis
    dd_pct = risk_profile.get('max_drawdown', 50)
    if dd_pct >= 70:
        dd_narrative = f"La **r√©silience exceptionnelle** face aux phases adverses (drawdown max: {abs(max_dd):.2f}%, score: {dd_pct:.0f}/100) d√©montre une capacit√© remarquable √† pr√©server le capital en p√©riode de stress, caract√©ristique essentielle pour la confiance des porteurs."
    elif dd_pct >= 40:
        dd_narrative = f"La **r√©silience mod√©r√©e** (drawdown max: {abs(max_dd):.2f}%, score: {dd_pct:.0f}/100) indique que le fonds a connu des p√©riodes de baisse significatives mais g√©rables, typiques d'une exposition aux actifs risqu√©s."
    else:
        dd_narrative = f"Les **drawdowns historiques importants** (max: {abs(max_dd):.2f}%, score: {dd_pct:.0f}/100) signalent un risque de perte en capital substantiel en p√©riode adverse, n√©cessitant une allocation prudente et une diversification appropri√©e."
    
    # Pain Ratio and investor experience
    if pain_ratio > 2:
        pain_narrative = f"Le **Pain Ratio exceptionnel** ({pain_ratio:.2f}) r√©v√®le que le fonds compense largement la 'douleur' ressentie par l'investisseur lors des phases de drawdown par ses performances, un attribut hautement valoris√© en gestion d'actifs."
    elif pain_ratio > 1:
        pain_narrative = f"Le **Pain Ratio positif** ({pain_ratio:.2f}) sugg√®re un √©quilibre acceptable entre les rendements g√©n√©r√©s et l'inconfort psychologique des p√©riodes de perte, caract√©ristique d'une gestion √©quilibr√©e."
    else:
        pain_narrative = f"Le **Pain Ratio limit√©** ({pain_ratio:.2f}) indique que la douleur ressentie lors des drawdowns n'est pas suffisamment compens√©e par la performance, un point d'attention pour la satisfaction des investisseurs."
    
    # Skewness and tail risk
    if skewness > 0.3:
        skew_narrative = f"L'**asym√©trie positive marqu√©e** (skewness: {skewness:.3f}) constitue un avantage significatif, avec un potentiel de gains extr√™mes sup√©rieur au risque de pertes catastrophiques - un profil recherch√© par les investisseurs avertis."
    elif abs(skewness) <= 0.3:
        skew_narrative = f"La **distribution relativement sym√©trique** des rendements (skewness: {skewness:.3f}) s'apparente √† une loi normale, sans biais particulier vers les queues de distribution."
    else:
        skew_narrative = f"L'**asym√©trie n√©gative** (skewness: {skewness:.3f}) constitue un signal d'alerte important, r√©v√©lant un risque accru de pertes extr√™mes ('tail risk') qui m√©rite une attention particuli√®re dans l'√©valuation du risque global."
    
    # Sharpe ratio interpretation
    if sharpe > 2:
        sharpe_text = f"un ratio de Sharpe exceptionnel ({sharpe:.2f})"
    elif sharpe > 1:
        sharpe_text = f"un ratio de Sharpe satisfaisant ({sharpe:.2f})"
    elif sharpe > 0:
        sharpe_text = f"un ratio de Sharpe modeste ({sharpe:.2f})"
    else:
        sharpe_text = f"un ratio de Sharpe n√©gatif ({sharpe:.2f}), sugg√©rant une sous-performance par rapport au taux sans risque"
    
    # Build comprehensive analysis
    analysis_parts = [
        opening,
        "",
        "**Analyse Multidimensionnelle du Risque :**",
        "",
        f"1. **Stabilit√© et Volatilit√©** : {vol_narrative}",
        "",
        f"2. **R√©silience et Drawdowns** : {dd_narrative}",
        "",
        f"3. **Exp√©rience Investisseur** : {pain_narrative} L'Ulcer Index de {ulcer:.2f} quantifie pr√©cis√©ment cette dimension.",
        "",
        f"4. **Profil Distributionnel** : {skew_narrative}",
        "",
        f"5. **Rendement Ajust√©** : Le fonds affiche {sharpe_text}, t√©moignant de {'son excellente' if sharpe > 2 else 'son' if sharpe > 1 else 'une'} capacit√© √† g√©n√©rer de la performance par unit√© de risque pris.",
        "",
    ]
    
    # Add strengths section
    if strengths:
        analysis_parts.extend([
            "**Points Forts Identifi√©s :**",
            ""
        ])
        for dim, score in strengths:
            if score >= 70:
                strength_desc = "excellent"
            elif score >= 60:
                strength_desc = "tr√®s bon"
            else:
                strength_desc = "bon"
            analysis_parts.append(f"- **{dim}** : Performance {strength_desc} (score: {score:.0f}/100)")
        analysis_parts.append("")
    
    # Add weaknesses section
    if weaknesses:
        analysis_parts.extend([
            "**Points d'Attention :**",
            ""
        ])
        for dim, score in weaknesses:
            if score < 30:
                weakness_desc = "n√©cessite une attention imm√©diate"
            elif score < 50:
                weakness_desc = "m√©riterait d'√™tre am√©lior√©"
            else:
                weakness_desc = "√† surveiller"
            analysis_parts.append(f"- **{dim}** : {weakness_desc} (score: {score:.0f}/100)")
        analysis_parts.append("")
    
    # Allocation recommendation
    analysis_parts.append("**Recommandation d'Allocation :**")
    analysis_parts.append("")
    if score_global >= 70:
        recommendation = f"Le profil de risque favorable de **{fcp_name}** permet d'envisager une **allocation significative** (15-25% d'un portefeuille diversifi√©), adapt√©e √† un large spectre d'investisseurs, y compris ceux recherchant un √©quilibre entre croissance et pr√©servation du capital."
    elif score_global >= 50:
        recommendation = f"Le profil √©quilibr√© sugg√®re une **allocation mod√©r√©e** (10-15% d'un portefeuille), en compl√©ment d'actifs plus d√©fensifs ou plus dynamiques selon les objectifs sp√©cifiques. Convient aux investisseurs avec une tol√©rance au risque moyenne √† √©lev√©e et un horizon moyen-long terme."
    else:
        recommendation = f"Le profil de risque √©lev√© recommande une **allocation limit√©e et tactique** (< 10% d'un portefeuille), strictement r√©serv√©e aux investisseurs aguerris avec une forte tol√©rance au risque, une capacit√© financi√®re appropri√©e, et un horizon d'investissement long terme."
    
    analysis_parts.append(recommendation)
    
    # Final synthesis
    analysis_parts.extend([
        "",
        "**Synth√®se D√©cisionnelle :**",
        ""
    ])
    
    if score_global >= 70:
        final_synthesis = f"**{fcp_name}** se positionne comme un v√©hicule d'investissement de qualit√© sup√©rieure, combinant ma√Ætrise des risques et potentiel de performance. La coh√©rence du profil de risque √† travers les diff√©rentes dimensions analys√©es renforce la confiance dans la stabilit√© future du fonds."
    elif score_global >= 50:
        final_synthesis = f"**{fcp_name}** pr√©sente un profil de risque acceptable dans sa cat√©gorie, avec un √©quilibre risque-rendement qui n√©cessite toutefois une surveillance active et une allocation r√©fl√©chie dans le cadre d'une strat√©gie de diversification appropri√©e."
    else:
        final_synthesis = f"**{fcp_name}** requiert une √©valuation approfondie des objectifs d'investissement et de la tol√©rance au risque avant toute allocation. Un suivi rapproch√© et des revues fr√©quentes sont indispensables, avec une pr√©paration aux sc√©narios de stress potentiels."
    
    analysis_parts.append(final_synthesis)
    
    return "\n".join(analysis_parts)

# Configuration de la page
st.set_page_config(
    page_title="Analyse FCP - Valeurs Liquidatives",
    page_icon="üìà",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for simplified styling
st.markdown(f"""
<style>
    .ranking-card {{
        background-color: #f8f9fa;
        padding: 0.5rem;
        border-radius: 3px;
        border: 1px solid #dee2e6;
        margin-bottom: 0.3rem;
    }}
    .ranking-card h3 {{
        color: {PRIMARY_COLOR};
        margin: 0 0 0.3rem 0;
        font-size: 1rem;
        font-weight: 600;
    }}
    .ranking-item {{
        background-color: #ffffff;
        padding: 0.3rem;
        border-radius: 2px;
        margin-bottom: 0.2rem;
        border: 1px solid #e9ecef;
    }}
    .ranking-number {{
        display: inline-block;
        background-color: {SECONDARY_COLOR};
        color: white;
        width: 24px;
        height: 24px;
        border-radius: 3px;
        text-align: center;
        line-height: 24px;
        margin-right: 5px;
        font-weight: bold;
        font-size: 0.85rem;
    }}
    .ranking-value {{
        float: right;
        font-weight: bold;
        font-size: 0.95rem;
    }}
    .insight-box {{
        background-color: #f8f9fa;
        border-left: 2px solid {PRIMARY_COLOR};
        padding: 0.5rem;
        border-radius: 3px;
        margin: 0.3rem 0;
    }}
    .insight-box h4 {{
        color: {PRIMARY_COLOR};
        margin: 0 0 0.3rem 0;
        font-size: 0.95rem;
    }}
    .interpretation-note {{
        background-color: #ffffff;
        border-left: 2px solid {SECONDARY_COLOR};
        padding: 0.5rem;
        border-radius: 3px;
        margin: 0.3rem 0;
        border: 1px solid #e9ecef;
    }}
    .alert-box {{
        background-color: #ffebee;
        border-left: 2px solid #dc3545;
        padding: 0.5rem;
        border-radius: 3px;
        margin: 0.3rem 0;
    }}
    .alert-box h4 {{
        color: #dc3545;
        margin: 0 0 0.3rem 0;
        font-size: 0.95rem;
    }}
    .alert-box p {{
        margin: 0.2rem 0;
    }}
    .alert-box ul {{
        margin: 0.2rem 0;
        padding-left: 1.2rem;
    }}
</style>
""", unsafe_allow_html=True)


def calculate_calendar_performance(df, fcp_name):
    """Calcule les performances calendaires (WTD, MTD, QTD, STD, YTD)"""
    latest_date = df['Date'].max()
    latest_value = df[df['Date'] == latest_date][fcp_name].values[0]
    
    # Week to Date
    week_start = latest_date - timedelta(days=latest_date.weekday())
    wtd_value = df[df['Date'] >= week_start][fcp_name].iloc[0] if len(df[df['Date'] >= week_start]) > 0 else latest_value
    wtd = ((latest_value / wtd_value) - 1) * 100
    
    # Month to Date
    month_start = latest_date.replace(day=1)
    mtd_value = df[df['Date'] >= month_start][fcp_name].iloc[0] if len(df[df['Date'] >= month_start]) > 0 else latest_value
    mtd = ((latest_value / mtd_value) - 1) * 100
    
    # Quarter to Date
    quarter_start = pd.Timestamp(latest_date.year, ((latest_date.month - 1) // 3) * 3 + 1, 1)
    qtd_value = df[df['Date'] >= quarter_start][fcp_name].iloc[0] if len(df[df['Date'] >= quarter_start]) > 0 else latest_value
    qtd = ((latest_value / qtd_value) - 1) * 100
    
    # Semester to Date
    semester_start = pd.Timestamp(latest_date.year, 1 if latest_date.month <= 6 else 7, 1)
    std_value = df[df['Date'] >= semester_start][fcp_name].iloc[0] if len(df[df['Date'] >= semester_start]) > 0 else latest_value
    std = ((latest_value / std_value) - 1) * 100
    
    # Year to Date
    year_start = pd.Timestamp(latest_date.year, 1, 1)
    ytd_value = df[df['Date'] >= year_start][fcp_name].iloc[0] if len(df[df['Date'] >= year_start]) > 0 else latest_value
    ytd = ((latest_value / ytd_value) - 1) * 100
    
    return {'WTD': wtd, 'MTD': mtd, 'QTD': qtd, 'STD': std, 'YTD': ytd}


def calculate_rolling_performance(df, fcp_name):
    """Calcule les performances glissantes"""
    latest_date = df['Date'].max()
    latest_value = df[df['Date'] == latest_date][fcp_name].values[0]
    
    performances = {}
    periods = {
        '1M': 30,
        '3M': 90,
        '6M': 180,
        '1Y': 365,
        '5Y': 1825
    }
    
    for label, days in periods.items():
        start_date = latest_date - timedelta(days=days)
        period_data = df[df['Date'] >= start_date]
        if len(period_data) > 0:
            start_value = period_data[fcp_name].iloc[0]
            performances[label] = ((latest_value / start_value) - 1) * 100
        else:
            performances[label] = None
    
    # Origine
    origin_value = df[fcp_name].iloc[0]
    performances['Origine'] = ((latest_value / origin_value) - 1) * 100
    
    return performances


def calculate_risk_metrics(df, fcp_name):
    """Calcule les indicateurs de risque avanc√©s"""
    returns = df[fcp_name].pct_change().dropna() * 100
    
    # M√©triques de base
    mean_return = returns.mean()
    volatility = returns.std()
    sharpe_ratio = (mean_return * TRADING_DAYS_PER_YEAR) / (volatility * np.sqrt(TRADING_DAYS_PER_YEAR)) if volatility > 0 else 0
    
    # VaR et CVaR (95%)
    var_95 = np.percentile(returns, 5)
    cvar_95 = returns[returns <= var_95].mean()
    
    # Skewness et Kurtosis
    skewness = stats.skew(returns)
    kurtosis = stats.kurtosis(returns)
    
    # Maximum Drawdown
    cumulative = (1 + df[fcp_name].pct_change().fillna(0)).cumprod()
    running_max = cumulative.expanding().max()
    drawdown = (cumulative - running_max) / running_max
    max_drawdown = drawdown.min() * 100
    
    return {
        'Rendement Moyen (%)': mean_return,
        'Volatilit√© (%)': volatility,
        'Ratio de Sharpe': sharpe_ratio,
        'VaR 95% (%)': var_95,
        'CVaR 95% (%)': cvar_95,
        'Skewness': skewness,
        'Kurtosis': kurtosis,
        'Max Drawdown (%)': max_drawdown
    }


def volatility_clustering(df, fcp_name, n_clusters=3, window=30):
    """Analyse les clusters de volatilit√© avec rolling window"""
    # Reset index to ensure proper alignment
    df_indexed = df.reset_index(drop=True)
    returns = df_indexed[fcp_name].pct_change() * 100
    
    # Volatilit√© glissante
    rolling_vol = returns.rolling(window=window).std()
    
    # Pr√©paration des donn√©es pour le clustering
    rolling_vol_clean = rolling_vol.dropna()
    X = rolling_vol_clean.values.reshape(-1, 1)
    
    # KMeans clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X)
    
    # Dates correspondantes: align with the rolling volatility indices
    dates = df_indexed['Date'].iloc[rolling_vol_clean.index].values
    
    return dates, rolling_vol_clean.values, clusters


def analyze_volatility_regimes(df, fcp_name, window=30, n_clusters=3):
    """
    Analyse avanc√©e des r√©gimes de volatilit√© avec interpr√©tation √©conomique
    
    Args:
        df: DataFrame with net asset values
        fcp_name: Name of the FCP
        window: Rolling window for volatility calculation (default: 30 days)
        n_clusters: Number of volatility regimes to identify (default: 3)
    
    Returns:
        dict: Dictionnaire contenant toutes les analyses de r√©gimes de volatilit√©
    """
    # Reset index pour alignement correct
    df_indexed = df.reset_index(drop=True)
    returns = df_indexed[fcp_name].pct_change() * 100
    
    # Calcul de la volatilit√© glissante
    rolling_vol = returns.rolling(window=window).std()
    rolling_vol_clean = rolling_vol.dropna()
    
    # Pr√©paration pour clustering
    X = rolling_vol_clean.values.reshape(-1, 1)
    
    # KMeans clustering with user-defined number of regimes
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    clusters = kmeans.fit_predict(X)
    
    # R√©cup√©ration des centres et lab√©lisation √©conomique
    centers = kmeans.cluster_centers_.flatten()
    cluster_order = np.argsort(centers)
    
    # Mapping √©conomique: ordre croissant de volatilit√© (0=plus faible, n-1=plus √©lev√©)
    regime_mapping = {cluster_order[i]: i for i in range(n_clusters)}
    labeled_clusters = np.array([regime_mapping[c] for c in clusters])
    
    # Generate regime names dynamically based on number of clusters
    if n_clusters == 2:
        regime_names = {0: "Faible Volatilit√©", 1: "Forte Volatilit√©"}
    elif n_clusters == 3:
        regime_names = {0: "Faible Volatilit√©", 1: "Volatilit√© Interm√©diaire", 2: "Forte Volatilit√©"}
    elif n_clusters == 4:
        regime_names = {0: "Tr√®s Faible Volatilit√©", 1: "Faible Volatilit√©", 2: "Volatilit√© √âlev√©e", 3: "Tr√®s Forte Volatilit√©"}
    elif n_clusters == 5:
        regime_names = {0: "Tr√®s Faible Volatilit√©", 1: "Faible Volatilit√©", 2: "Volatilit√© Mod√©r√©e", 3: "Volatilit√© √âlev√©e", 4: "Tr√®s Forte Volatilit√©"}
    else:
        regime_names = {i: f"R√©gime {i+1}" for i in range(n_clusters)}
    
    # Dates correspondantes
    dates = df_indexed['Date'].iloc[rolling_vol_clean.index].values
    
    # Cr√©ation d'un DataFrame avec tous les indices align√©s
    regime_df = pd.DataFrame({
        'Date': dates,
        'Volatility': rolling_vol_clean.values,
        'Regime': labeled_clusters
    })
    
    # Ajout des rendements align√©s
    aligned_returns = returns.iloc[rolling_vol_clean.index].values
    regime_df['Return'] = aligned_returns
    
    # Calcul du drawdown pour chaque point
    cumulative = (1 + df_indexed[fcp_name].pct_change().fillna(0)).cumprod()
    running_max = cumulative.expanding().max()
    drawdown = (cumulative - running_max) / running_max * 100
    aligned_drawdown = drawdown.iloc[rolling_vol_clean.index].values
    regime_df['Drawdown'] = aligned_drawdown
    
    # Statistiques par r√©gime
    regime_stats = {}
    for regime_id in range(n_clusters):
        regime_data = regime_df[regime_df['Regime'] == regime_id]
        
        regime_stats[regime_id] = {
            'name': regime_names[regime_id],
            'count': len(regime_data),
            'proportion': len(regime_data) / len(regime_df) * 100,
            'avg_volatility': regime_data['Volatility'].mean(),
            'avg_return': regime_data['Return'].mean(),
            'max_drawdown': regime_data['Drawdown'].min(),
            'min_volatility': regime_data['Volatility'].min(),
            'max_volatility': regime_data['Volatility'].max(),
        }
    
    # Analyse des transitions entre r√©gimes
    transitions = np.zeros((n_clusters, n_clusters))
    for i in range(len(labeled_clusters) - 1):
        from_regime = labeled_clusters[i]
        to_regime = labeled_clusters[i + 1]
        transitions[from_regime, to_regime] += 1
    
    # Normalisation pour obtenir des probabilit√©s
    transition_probs = transitions / transitions.sum(axis=1, keepdims=True)
    transition_probs = np.nan_to_num(transition_probs)  # Remplacer NaN par 0
    
    # R√©gime actuel
    current_regime = labeled_clusters[-1]
    current_regime_name = regime_names[current_regime]
    
    # Analyse de persistance (temps moyen dans chaque r√©gime)
    regime_sequences = []
    current_seq = {'regime': labeled_clusters[0], 'length': 1}
    
    for i in range(1, len(labeled_clusters)):
        if labeled_clusters[i] == current_seq['regime']:
            current_seq['length'] += 1
        else:
            regime_sequences.append(current_seq)
            current_seq = {'regime': labeled_clusters[i], 'length': 1}
    regime_sequences.append(current_seq)
    
    # Calcul de la persistance moyenne par r√©gime
    persistence = {}
    for regime_id in range(n_clusters):
        regime_lengths = [seq['length'] for seq in regime_sequences if seq['regime'] == regime_id]
        persistence[regime_id] = {
            'avg_duration': np.mean(regime_lengths) if regime_lengths else 0,
            'max_duration': np.max(regime_lengths) if regime_lengths else 0,
            'episodes': len(regime_lengths)
        }
    
    # Analyse risque-rendement par r√©gime
    risk_return_analysis = {}
    for regime_id in range(n_clusters):
        regime_data = regime_df[regime_df['Regime'] == regime_id]
        if len(regime_data) > 0:
            # Returns are in percentage, convert to decimal for Sharpe ratio calculation
            mean_return_decimal = regime_data['Return'].mean() / 100
            std_return_decimal = regime_data['Return'].std() / 100
            sharpe = (mean_return_decimal * TRADING_DAYS_PER_YEAR) / \
                     (std_return_decimal * np.sqrt(TRADING_DAYS_PER_YEAR)) \
                     if std_return_decimal > 0 else 0
            
            risk_return_analysis[regime_id] = {
                'sharpe_ratio': sharpe,
                'return_volatility_ratio': regime_data['Return'].mean() / regime_data['Volatility'].mean() \
                                          if regime_data['Volatility'].mean() > 0 else 0
            }
    
    return {
        'regime_df': regime_df,
        'regime_stats': regime_stats,
        'regime_names': regime_names,
        'transitions': transitions,
        'transition_probs': transition_probs,
        'current_regime': current_regime,
        'current_regime_name': current_regime_name,
        'persistence': persistence,
        'risk_return_analysis': risk_return_analysis,
        'sorted_centers': sorted(centers)
    }


def analyze_drawdowns(df, fcp_name):
    """
    Analyse dynamique des drawdowns: profondeur, fr√©quence, dur√©e et temps de r√©cup√©ration
    
    Returns:
        dict: M√©triques de drawdown incluant profils des √©pisodes de stress
    """
    df_indexed = df.reset_index(drop=True)
    prices = df_indexed[fcp_name]
    
    # Calcul du drawdown s√©rie compl√®te
    cumulative = (1 + prices.pct_change().fillna(0)).cumprod()
    running_max = cumulative.expanding().max()
    drawdown = (cumulative - running_max) / running_max * 100
    
    # Identification des √©pisodes de drawdown
    in_drawdown = drawdown < 0
    drawdown_episodes = []
    
    if in_drawdown.any():
        # Trouver les d√©buts et fins d'√©pisodes
        starts = in_drawdown.ne(in_drawdown.shift()).cumsum()
        for episode_id in starts[in_drawdown].unique():
            episode_mask = (starts == episode_id) & in_drawdown
            episode_data = drawdown[episode_mask]
            
            if len(episode_data) > 0:
                start_idx = episode_data.index[0]
                end_idx = episode_data.index[-1]
                
                # Trouver le temps de r√©cup√©ration
                recovery_idx = None
                if end_idx < len(drawdown) - 1:
                    future_dd = drawdown.iloc[end_idx+1:]
                    recovery_points = future_dd[future_dd >= -0.01]  # R√©cup√©ration √† <0.01% du max
                    if len(recovery_points) > 0:
                        recovery_idx = recovery_points.index[0]
                
                drawdown_episodes.append({
                    'start_date': df_indexed['Date'].iloc[start_idx],
                    'end_date': df_indexed['Date'].iloc[end_idx],
                    'start_idx': start_idx,
                    'end_idx': end_idx,
                    'recovery_idx': recovery_idx,
                    'depth': episode_data.min(),
                    'duration': len(episode_data),
                    'recovery_time': (recovery_idx - end_idx) if recovery_idx is not None else None
                })
    
    # Calcul des m√©triques globales
    max_dd = drawdown.min()
    avg_dd = drawdown[drawdown < 0].mean() if (drawdown < 0).any() else 0
    
    # Ulcer Index: racine carr√©e de la moyenne des drawdowns au carr√©
    ulcer_index = np.sqrt((drawdown ** 2).mean())
    
    # Pain Ratio: rendement total / Ulcer Index
    total_return = ((prices.iloc[-1] / prices.iloc[0]) - 1) * 100
    pain_ratio = total_return / ulcer_index if ulcer_index > 0 else 0
    
    return {
        'max_drawdown': max_dd,
        'avg_drawdown': avg_dd,
        'ulcer_index': ulcer_index,
        'pain_ratio': pain_ratio,
        'drawdown_series': drawdown,
        'drawdown_episodes': drawdown_episodes,
        'num_episodes': len(drawdown_episodes),
        'dates': df_indexed['Date']
    }


def calculate_rolling_risk_indicators(df, fcp_name, window=60):
    """
    Calcule les indicateurs de risque rolling pour d√©tecter les √©volutions du profil de risque
    
    Args:
        df: DataFrame avec les VL
        fcp_name: Nom du FCP
        window: Fen√™tre pour le rolling (d√©faut 60 jours ‚âà 3 mois)
    
    Returns:
        DataFrame avec les indicateurs rolling
    """
    df_indexed = df.reset_index(drop=True)
    returns = df_indexed[fcp_name].pct_change() * 100
    
    # Rolling metrics
    rolling_mean = returns.rolling(window=window).mean()
    rolling_std = returns.rolling(window=window).std()
    
    # Rolling Sharpe Ratio
    rolling_sharpe = (rolling_mean * TRADING_DAYS_PER_YEAR) / \
                     (rolling_std * np.sqrt(TRADING_DAYS_PER_YEAR))
    
    # Rolling VaR et CVaR
    rolling_var = returns.rolling(window=window).quantile(0.05)
    
    # Rolling CVaR (mean of returns below VaR)
    def calc_cvar(x):
        if len(x) < 2:
            return np.nan
        var = np.percentile(x.dropna(), 5)
        return x[x <= var].mean()
    
    rolling_cvar = returns.rolling(window=window).apply(calc_cvar, raw=False)
    
    # Construire le DataFrame de r√©sultats
    rolling_df = pd.DataFrame({
        'Date': df_indexed['Date'],
        'Rolling_Sharpe': rolling_sharpe,
        'Rolling_VaR': rolling_var,
        'Rolling_CVaR': rolling_cvar,
        'Rolling_Volatility': rolling_std
    })
    
    return rolling_df


def calculate_loss_probabilities(df, fcp_name):
    """
    Calcule les probabilit√©s empiriques de perte √† 1, 3 et 6 mois
    
    Returns:
        dict: Probabilit√©s de perte √† diff√©rents horizons
    """
    df_indexed = df.reset_index(drop=True)
    prices = df_indexed[fcp_name]
    
    horizons = {
        '1M': 21,   # ~1 mois
        '3M': 63,   # ~3 mois
        '6M': 126   # ~6 mois
    }
    
    loss_probs = {}
    
    for label, days in horizons.items():
        returns_horizon = []
        
        # Calculer les rendements √† l'horizon donn√©
        for i in range(len(prices) - days):
            ret = ((prices.iloc[i + days] / prices.iloc[i]) - 1) * 100
            returns_horizon.append(ret)
        
        if len(returns_horizon) > 0:
            returns_arr = np.array(returns_horizon)
            loss_prob = (returns_arr < 0).sum() / len(returns_arr) * 100
            avg_loss = returns_arr[returns_arr < 0].mean() if (returns_arr < 0).any() else 0
            avg_gain = returns_arr[returns_arr > 0].mean() if (returns_arr > 0).any() else 0
            
            loss_probs[label] = {
                'probability': loss_prob,
                'avg_loss': avg_loss,
                'avg_gain': avg_gain,
                'gain_loss_ratio': abs(avg_gain / avg_loss) if avg_loss < 0 else 0
            }
        else:
            loss_probs[label] = {
                'probability': 0,
                'avg_loss': 0,
                'avg_gain': 0,
                'gain_loss_ratio': 0
            }
    
    return loss_probs


def calculate_capture_ratios(df, fcp_name, benchmark_name=None):
    """
    Calcule les ratios de capture haussi√®re et baissi√®re
    Si pas de benchmark, utilise la moyenne de tous les FCP
    
    Returns:
        dict: Ratios de capture up/down
    """
    df_indexed = df.reset_index(drop=True)
    fcp_returns = df_indexed[fcp_name].pct_change().dropna() * 100
    
    # Si pas de benchmark, utiliser la moyenne des autres FCP
    if benchmark_name is None or benchmark_name not in df_indexed.columns:
        fcp_cols = [col for col in df_indexed.columns if col.startswith('FCP') and col != fcp_name]
        if len(fcp_cols) > 0:
            benchmark_returns = df_indexed[fcp_cols].pct_change().mean(axis=1).dropna() * 100
        else:
            return {'upside_capture': 0, 'downside_capture': 0}
    else:
        benchmark_returns = df_indexed[benchmark_name].pct_change().dropna() * 100
    
    # Aligner les s√©ries
    min_len = min(len(fcp_returns), len(benchmark_returns))
    fcp_returns = fcp_returns.iloc[:min_len]
    benchmark_returns = benchmark_returns.iloc[:min_len]
    
    # P√©riodes haussi√®res et baissi√®res du benchmark
    up_periods = benchmark_returns > 0
    down_periods = benchmark_returns < 0
    
    # Capture ratios
    if up_periods.any():
        upside_capture = (fcp_returns[up_periods].mean() / benchmark_returns[up_periods].mean()) * 100
    else:
        upside_capture = 0
    
    if down_periods.any():
        downside_capture = (fcp_returns[down_periods].mean() / benchmark_returns[down_periods].mean()) * 100
    else:
        downside_capture = 0
    
    return {
        'upside_capture': upside_capture,
        'downside_capture': downside_capture
    }


def calculate_risk_fingerprint(df, fcp_name):
    """
    Construit une signature synth√©tique du risque normalis√©e et comparable
    
    Returns:
        dict: M√©triques normalis√©es pour le profil de risque
    """
    # Calcul des m√©triques de base
    returns = df[fcp_name].pct_change().dropna() * 100
    volatility = returns.std()
    
    # Drawdown analysis
    dd_analysis = analyze_drawdowns(df, fcp_name)
    
    # Rolling indicators pour la stabilit√© du Sharpe
    rolling_risk = calculate_rolling_risk_indicators(df, fcp_name, window=60)
    sharpe_stability = rolling_risk['Rolling_Sharpe'].std()
    
    # CVaR pour le risque extr√™me
    var_95 = np.percentile(returns, 5)
    cvar_95 = returns[returns <= var_95].mean()
    
    # Asym√©trie
    skewness = stats.skew(returns)
    
    # Temps de r√©cup√©ration moyen
    episodes_with_recovery = [ep for ep in dd_analysis['drawdown_episodes'] 
                             if ep['recovery_time'] is not None]
    avg_recovery_time = np.mean([ep['recovery_time'] for ep in episodes_with_recovery]) \
                       if episodes_with_recovery else 0
    
    return {
        'volatility': volatility,
        'max_drawdown': abs(dd_analysis['max_drawdown']),
        'avg_drawdown': abs(dd_analysis['avg_drawdown']),
        'avg_recovery_time': avg_recovery_time,
        'cvar_95': abs(cvar_95),
        'skewness': skewness,
        'sharpe_stability': sharpe_stability,
        'ulcer_index': dd_analysis['ulcer_index'],
        'pain_ratio': dd_analysis['pain_ratio']
    }


def normalize_risk_fingerprint(fingerprints_dict):
    """
    Normalise les fingerprints pour comparaison entre FCP
    
    Args:
        fingerprints_dict: Dict avec {fcp_name: fingerprint}
    
    Returns:
        dict: Fingerprints normalis√©s [0-100]
    """
    if not fingerprints_dict:
        return {}
    
    # Extraire toutes les valeurs par m√©trique
    metrics = list(next(iter(fingerprints_dict.values())).keys())
    
    normalized = {}
    
    for fcp_name, fingerprint in fingerprints_dict.items():
        normalized[fcp_name] = {}
        
        for metric in metrics:
            values = [fp[metric] for fp in fingerprints_dict.values()]
            min_val = min(values)
            max_val = max(values)
            
            # Normalisation [0-100]
            if max_val > min_val:
                norm_value = ((fingerprint[metric] - min_val) / (max_val - min_val)) * 100
            else:
                norm_value = 50  # Valeur m√©diane si tous √©gaux
            
            # Inverser pour les m√©triques "moins c'est mieux"
            if metric in ['volatility', 'max_drawdown', 'avg_drawdown', 'avg_recovery_time', 
                         'cvar_95', 'sharpe_stability', 'ulcer_index']:
                norm_value = 100 - norm_value
            
            # Ajuster skewness (positif = bien, n√©gatif = mauvais)
            if metric == 'skewness':
                # Convertir de [-inf, +inf] vers [0, 100]
                # Skewness positif (queues √† droite) = mieux
                if fingerprint[metric] >= 0:
                    norm_value = 50 + min(fingerprint[metric] * 10, 50)
                else:
                    norm_value = 50 + max(fingerprint[metric] * 10, -50)
            
            normalized[fcp_name][metric] = norm_value
    
    return normalized


def calculate_7d_risk_profile(df, fcp_name):
    """
    Calcule le profil de risque sur 7 dimensions pour le Risk Fingerprint.
    
    Les 7 dimensions sont:
    a. Stabilit√©: Inverse de la volatilit√© (plus haut = plus stable)
    b. R√©silience: Inverse du max drawdown (plus haut = plus r√©silient)
    c. R√©cup√©ration: Inverse du temps de r√©cup√©ration moyen
    d. Protection Extr√™me: Inverse de la CVaR (plus haut = mieux prot√©g√©)
    e. Asym√©trie: Skewness normalis√©e (plus haut = meilleure asym√©trie)
    f. Sharpe Stable: Stabilit√© du ratio de Sharpe dans le temps
    g. Pain Ratio: Rendement ajust√© √† la douleur
    
    Args:
        df: DataFrame avec les valeurs liquidatives
        fcp_name: Nom du FCP
    
    Returns:
        dict: Les 7 dimensions avec leurs valeurs brutes
    """
    # Calcul des m√©triques de base
    returns = df[fcp_name].pct_change().dropna() * 100
    
    # 1. Stabilit√© (inverse de la volatilit√©)
    volatility = returns.std()
    stabilite = volatility  # Sera invers√© lors de la normalisation
    
    # 2. R√©silience (inverse du max drawdown)
    dd_analysis = analyze_drawdowns(df, fcp_name)
    resilience = abs(dd_analysis['max_drawdown'])  # Sera invers√© lors de la normalisation
    
    # 3. R√©cup√©ration (inverse du temps de r√©cup√©ration moyen)
    episodes_with_recovery = [ep for ep in dd_analysis['drawdown_episodes'] 
                             if ep['recovery_time'] is not None]
    # Use minimum meaningful recovery time (1 day) if no recovery episodes exist
    # This prevents division by zero and represents "instant recovery" scenario
    avg_recovery_time = np.mean([ep['recovery_time'] for ep in episodes_with_recovery]) \
                       if episodes_with_recovery else 1
    recuperation = avg_recovery_time  # Sera invers√© lors de la normalisation
    
    # 4. Protection Extr√™me (inverse de la CVaR)
    var_95 = np.percentile(returns, 5)
    cvar_95 = returns[returns <= var_95].mean()
    protection_extreme = abs(cvar_95)  # Sera invers√© lors de la normalisation
    
    # 5. Asym√©trie (skewness normalis√©e)
    skewness = stats.skew(returns)
    asymetrie = skewness  # Sera trait√© sp√©cialement lors de la normalisation
    
    # 6. Sharpe Stable (stabilit√© du ratio de Sharpe)
    rolling_risk = calculate_rolling_risk_indicators(df, fcp_name, window=60)
    sharpe_stability = rolling_risk['Rolling_Sharpe'].std()
    sharpe_stable = sharpe_stability  # Sera invers√© lors de la normalisation
    
    # 7. Pain Ratio (rendement ajust√© √† la douleur)
    pain_ratio = dd_analysis['pain_ratio']
    
    return {
        'Stabilit√©': stabilite,
        'R√©silience': resilience,
        'R√©cup√©ration': recuperation,
        'Protection Extr√™me': protection_extreme,
        'Asym√©trie': asymetrie,
        'Sharpe Stable': sharpe_stable,
        'Pain Ratio': pain_ratio
    }


def normalize_7d_risk_profile(profiles_dict):
    """
    Normalise les profils de risque 7D pour comparaison entre FCP.
    Score normalis√© = (Valeur - Min) / (Max - Min) √ó 100
    
    Args:
        profiles_dict: Dict avec {fcp_name: profile_7d}
    
    Returns:
        dict: Profils normalis√©s [0-100]
    """
    if not profiles_dict:
        return {}
    
    # Dimensions √† inverser (moins c'est mieux)
    inverse_dimensions = ['Stabilit√©', 'R√©silience', 'R√©cup√©ration', 'Protection Extr√™me', 'Sharpe Stable']
    
    normalized = {}
    
    for fcp_name, profile in profiles_dict.items():
        normalized[fcp_name] = {}
        
        for dimension, value in profile.items():
            # Collecter toutes les valeurs pour cette dimension
            all_values = [p[dimension] for p in profiles_dict.values()]
            min_val = min(all_values)
            max_val = max(all_values)
            
            # Normalisation [0-100]
            if max_val > min_val:
                norm_value = ((value - min_val) / (max_val - min_val)) * 100
            else:
                norm_value = 50  # Valeur m√©diane si tous √©gaux
            
            # Inverser pour les dimensions "moins c'est mieux"
            if dimension in inverse_dimensions:
                norm_value = 100 - norm_value
            
            # Traitement sp√©cial pour l'asym√©trie (skewness)
            if dimension == 'Asym√©trie':
                # Convertir de [-inf, +inf] vers [0, 100]
                # Skewness positif (queues √† droite) = mieux, maps to [50, 100]
                # Skewness n√©gatif (queues √† gauche) = moins bien, maps to [0, 50]
                # Utilise SKEWNESS_SCALE_FACTOR pour la transformation lin√©aire
                if value >= 0:
                    norm_value = SKEWNESS_NEUTRAL_SCORE + min(value * SKEWNESS_SCALE_FACTOR, SKEWNESS_NEUTRAL_SCORE)
                else:
                    norm_value = SKEWNESS_NEUTRAL_SCORE + max(value * SKEWNESS_SCALE_FACTOR, -SKEWNESS_NEUTRAL_SCORE)
            
            normalized[fcp_name][dimension] = norm_value
    
    return normalized


def create_risk_fingerprint_chart(normalized_profile, fcp_name):
    """
    Cr√©e un radar chart (spider chart) pour le Risk Fingerprint.
    
    Args:
        normalized_profile: Dict avec les 7 dimensions normalis√©es [0-100]
        fcp_name: Nom du FCP
    
    Returns:
        plotly.graph_objects.Figure: Le radar chart
    """
    dimensions = list(normalized_profile.keys())
    values = list(normalized_profile.values())
    
    # Fermer le radar chart en ajoutant la premi√®re valeur √† la fin
    dimensions_closed = dimensions + [dimensions[0]]
    values_closed = values + [values[0]]
    
    fig = go.Figure()
    
    fig.add_trace(go.Scatterpolar(
        r=values_closed,
        theta=dimensions_closed,
        fill='toself',
        fillcolor=hex_to_rgba(PRIMARY_COLOR, 0.3),
        line=dict(color=PRIMARY_COLOR, width=2),
        name=fcp_name,
        hovertemplate='<b>%{theta}</b><br>Score: %{r:.1f}/100<extra></extra>'
    ))
    
    fig.update_layout(
        polar=dict(
            radialaxis=dict(
                visible=True,
                range=[0, 100],
                tickmode='linear',
                tick0=0,
                dtick=20,
                showticklabels=True,
                ticks='outside'
            ),
            angularaxis=dict(
                direction='clockwise'
            )
        ),
        showlegend=True,
        title=f"Risk Fingerprint - {fcp_name}",
        height=500,
        template="plotly_white"
    )
    
    return fig


@st.cache_data
def load_data():
    """Charge les donn√©es du fichier CSV ou Excel"""
    file_extension = os.path.splitext(DATA_FILE)[1].lower()
    
    if file_extension == '.csv':
        df = pd.read_csv(DATA_FILE)
    else:
        df = pd.read_excel(DATA_FILE, sheet_name='Valeurs Liquidatives')
    
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.sort_values('Date')
    return df


def main():
    """Main function for the Valeurs Liquidatives page"""
    st.header("üìà Analyse des Valeurs Liquidatives")
    
    # Load data
    with st.spinner('Chargement des donn√©es...'):
        df = load_data()
    
    # Obtenir la liste des FCP
    fcp_cols = [col for col in df.columns if col != 'Date']
    
    # Sidebar pour les filtres
    with st.sidebar:
        st.header("üîß Filtres et Param√®tres")
        
        # S√©lection du/des FCP
        st.markdown("### üìä S√©lection des FCP")
        
        # Initialize session state for selected FCPs if not exists
        # Default to empty list (which will select all FCPs)
        if 'vl_selected_fcps' not in st.session_state:
            st.session_state.vl_selected_fcps = []

        selected_fcps = st.multiselect(
            "FCP √† analyser",
            options=fcp_cols,
            default=st.session_state.vl_selected_fcps,
            key="vl_multiselect",
            help="S√©lectionnez un ou plusieurs FCP. Si aucun FCP n'est s√©lectionn√©, tous les FCP seront analys√©s."
        )

        
        # Update session state
        st.session_state.vl_selected_fcps = selected_fcps
        
        # If no FCP is selected, use all FCPs
        if not selected_fcps:
            selected_fcps = fcp_cols
            st.info(f"üìå **Tous les FCP s√©lectionn√©s ({len(fcp_cols)} FCP)**")
        else:
            st.info(f"üìå **{len(selected_fcps)}/{len(fcp_cols)}** FCP s√©lectionn√©s")
        
        # Filtre de date avec options rapides
        with st.expander("üìÖ P√©riode d'analyse", expanded=True):
            quick_filter = st.radio(
                "Filtres rapides",
                options=['Personnalis√©', 'WTD', 'MTD', 'QTD', 'YTD', 'Origine'],
                index=5,
                help="WTD: Semaine, MTD: Mois, QTD: Trimestre, YTD: Ann√©e, Origine: Depuis le d√©but",
                horizontal=True
            )
            
            min_date = df['Date'].min()
            max_date = df['Date'].max()
            
            if quick_filter == 'WTD':
                date_range = (max_date - timedelta(days=max_date.weekday()), max_date)
            elif quick_filter == 'MTD':
                date_range = (max_date.replace(day=1), max_date)
            elif quick_filter == 'QTD':
                quarter_start_month = ((max_date.month - 1) // 3) * 3 + 1
                date_range = (max_date.replace(month=quarter_start_month, day=1), max_date)
            elif quick_filter == 'YTD':
                date_range = (max_date.replace(month=1, day=1), max_date)
            elif quick_filter == 'Origine':
                date_range = (min_date, max_date)
            else:
                date_range = st.date_input(
                    "S√©lectionnez la p√©riode",
                    value=(df['Date'].min(), df['Date'].max()),
                    min_value=df['Date'].min(),
                    max_value=df['Date'].max(),
                    key="valeurs_liquidatives_date_range"
                )
            
            # Display selected date range
            if isinstance(date_range, tuple) and len(date_range) == 2:
                try:
                    st.caption(f"üìÖ Du {date_range[0].strftime('%d/%m/%Y')} au {date_range[1].strftime('%d/%m/%Y')}")
                except (AttributeError, TypeError):
                    pass
        
        # Param√®tres d'analyse de volatilit√©
        with st.expander("‚öôÔ∏è Param√®tres d'Analyse de Volatilit√©", expanded=False):
            st.markdown("**Configuration des fen√™tres d'analyse**")
            
            # Initialize session state for volatility parameters if not exists
            if 'vl_volatility_window' not in st.session_state:
                st.session_state.vl_volatility_window = 30
            if 'vl_rolling_risk_window' not in st.session_state:
                st.session_state.vl_rolling_risk_window = 60
            if 'vl_n_clusters' not in st.session_state:
                st.session_state.vl_n_clusters = 3
            
            volatility_window = st.slider(
                "Fen√™tre de volatilit√© (jours)",
                min_value=5,
                max_value=120,
                value=st.session_state.vl_volatility_window,
                step=5,
                key="vl_vol_slider",
                help="P√©riode glissante pour le calcul de la volatilit√© et l'analyse des r√©gimes. Une fen√™tre plus courte (5-20 jours) d√©tecte les changements rapides, une fen√™tre plus longue (60-120 jours) lisse les variations."
            )
            st.session_state.vl_volatility_window = volatility_window
            
            rolling_risk_window = st.slider(
                "Fen√™tre d'indicateurs de risque (jours)",
                min_value=20,
                max_value=180,
                value=st.session_state.vl_rolling_risk_window,
                step=10,
                key="vl_risk_slider",
                help="P√©riode pour les indicateurs de risque rolling (Sharpe, VaR, CVaR). G√©n√©ralement 2-3 fois la fen√™tre de volatilit√© pour une analyse plus stable."
            )
            st.session_state.vl_rolling_risk_window = rolling_risk_window
            
            n_clusters = st.slider(
                "Nombre de r√©gimes de volatilit√©",
                min_value=2,
                max_value=5,
                value=st.session_state.vl_n_clusters,
                step=1,
                key="vl_clusters_slider",
                help="Nombre de r√©gimes de volatilit√© √† identifier (2 = faible/√©lev√©, 3 = faible/interm√©diaire/√©lev√©, etc.). Valeur recommand√©e : 3."
            )
            st.session_state.vl_n_clusters = n_clusters
            
            # Display current configuration
            st.caption(f"üìä Configuration actuelle:")
            st.caption(f"‚Ä¢ Volatilit√©: fen√™tre de {volatility_window} jours")
            st.caption(f"‚Ä¢ Risque: fen√™tre de {rolling_risk_window} jours")
            st.caption(f"‚Ä¢ R√©gimes: {n_clusters} clusters")
            
            # Reset to defaults button
            if st.button("üîÑ R√©initialiser aux valeurs par d√©faut", key="vl_reset_volatility_params", use_container_width=True):
                st.session_state.vl_volatility_window = 30
                st.session_state.vl_rolling_risk_window = 60
                st.session_state.vl_n_clusters = 3
                st.rerun()
        
        # Save/Load selections
        with st.expander("üíæ Sauvegarder/Charger la S√©lection", expanded=False):
            # Save current selection
            selection_name = st.text_input("Nom de la s√©lection", key="vl_save_name")
            if st.button("üíæ Sauvegarder la s√©lection actuelle", use_container_width=True):
                if selection_name:
                    if 'vl_saved_selections' not in st.session_state:
                        st.session_state.vl_saved_selections = {}
                    # Save the actual selected FCPs from session state (before conversion to all FCPs)
                    st.session_state.vl_saved_selections[selection_name] = st.session_state.vl_selected_fcps.copy()
                    st.success(f"‚úÖ S√©lection '{selection_name}' sauvegard√©e!")
                else:
                    st.warning("‚ö†Ô∏è Veuillez entrer un nom pour la s√©lection")
            
            # Load saved selection
            if 'vl_saved_selections' in st.session_state and st.session_state.vl_saved_selections:
                saved_names = list(st.session_state.vl_saved_selections.keys())
                selected_save = st.selectbox("Charger une s√©lection", options=[""] + saved_names)
                
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("üìÇ Charger", use_container_width=True) and selected_save:
                        st.session_state.vl_selected_fcps = st.session_state.vl_saved_selections[selected_save]
                        st.rerun()
                with col2:
                    if st.button("üóëÔ∏è Supprimer", use_container_width=True) and selected_save:
                        del st.session_state.vl_saved_selections[selected_save]
                        st.rerun()
            else:
                st.info("Aucune s√©lection sauvegard√©e")
        
        # Bouton de r√©initialisation
        if st.button("üîÑ R√©initialiser tous les filtres", use_container_width=True):
            st.session_state.vl_selected_fcps = []
            st.rerun()
    
    # Note: removed the check "if not selected_fcps" since empty selection now means "all FCPs"
    
    # Filtrage des donn√©es par date (used for some analyses but not for performances)
    if len(date_range) == 2:
        mask = (df['Date'] >= pd.Timestamp(date_range[0])) & (df['Date'] <= pd.Timestamp(date_range[1]))
        filtered_df = df[mask].copy()
    else:
        filtered_df = df.copy()
    
    # Use full data for performance calculations
    full_df = df.copy()
    
    # ===================
    # Section 0: Comparative Summary - Selected vs All FCPs
    # ===================
    st.markdown("### üìä Vue d'Ensemble Comparative")
    
    # Calculate stats for all FCPs
    all_fcp_stats = {}
    for fcp in fcp_cols:
        all_fcp_stats[fcp] = {
            'current': filtered_df[fcp].iloc[-1],
            'initial': filtered_df[fcp].iloc[0],
            'performance': ((filtered_df[fcp].iloc[-1] / filtered_df[fcp].iloc[0]) - 1) * 100,
            'volatility': (filtered_df[fcp].pct_change().dropna() * 100).std()
        }
    

    # Positioning chart - where selected FCPs rank among all
    st.markdown("#### üìç Positionnement des FCP S√©lectionn√©s")
    
    # Create ranking dataframe
    ranking_df = pd.DataFrame([
        {
            'FCP': fcp,
            'Performance (%)': all_fcp_stats[fcp]['performance'],
            'Volatilit√© (%)': all_fcp_stats[fcp]['volatility'],
            'S√©lectionn√©': 'Oui' if fcp in selected_fcps else 'Non'
        }
        for fcp in fcp_cols
    ]).sort_values('Performance (%)', ascending=False)
    
    # Add rank
    ranking_df['Rang'] = range(1, len(ranking_df) + 1)
    
    # Create scatter plot showing performance vs volatility
    fig_positioning = go.Figure()
    
    # Plot all FCPs
    for _, row in ranking_df.iterrows():
        is_selected = row['S√©lectionn√©'] == 'Oui'
        fig_positioning.add_trace(go.Scatter(
            x=[row['Volatilit√© (%)']],
            y=[row['Performance (%)']],
            mode='markers+text',
            name=row['FCP'],
            text=row['FCP'],
            textposition='top center',
            marker=dict(
                size=15 if is_selected else 8,
                color=PRIMARY_COLOR if is_selected else SECONDARY_COLOR,
                opacity=1.0 if is_selected else 0.4,
                line=dict(width=2 if is_selected else 0, color='white')
            ),
            textfont=dict(
                size=10 if is_selected else 8,
                color=PRIMARY_COLOR if is_selected else SECONDARY_COLOR
            ),
            showlegend=False,
            hovertemplate='<b>%{text}</b><br>' +
                         'Performance: %{y:.2f}%<br>' +
                         'Volatilit√©: %{x:.2f}%<br>' +
                         '<extra></extra>'
        ))
    
    # Add quadrant lines (median)
    median_perf = ranking_df['Performance (%)'].median()
    median_vol = ranking_df['Volatilit√© (%)'].median()
    
    fig_positioning.add_hline(y=median_perf, line_dash="dash", line_color="gray", opacity=0.3)
    fig_positioning.add_vline(x=median_vol, line_dash="dash", line_color="gray", opacity=0.3)
    
    fig_positioning.update_layout(
        title="Performance vs Volatilit√© - FCP S√©lectionn√©s en surbrillance",
        xaxis_title="Volatilit√© (%)",
        yaxis_title="Performance (%)",
        height=500,
        template="plotly_white",
        hovermode='closest'
    )
    
    st.plotly_chart(fig_positioning, use_container_width=True)
    
    # Show ranking table for selected FCPs (without Rang column)
    selected_ranking = ranking_df[ranking_df['S√©lectionn√©'] == 'Oui'][['FCP', 'Performance (%)', 'Volatilit√© (%)']].reset_index(drop=True)
    
    st.markdown("#### üìã Classement des FCP S√©lectionn√©s")
    
    st.dataframe(
        selected_ranking.style.background_gradient(subset=['Performance (%)'], cmap='RdYlGn')
                          .background_gradient(subset=['Volatilit√© (%)'], cmap='RdYlGn_r')
                          .format({'Performance (%)': '{:+.2f}%', 'Volatilit√© (%)': '{:.2f}%'}),
        use_container_width=True,
        hide_index=True
    )
    
    # Add brief comment based on results
    best_fcp = selected_ranking.loc[selected_ranking['Performance (%)'].idxmax()]
    worst_fcp = selected_ranking.loc[selected_ranking['Performance (%)'].idxmin()]
    avg_perf = selected_ranking['Performance (%)'].mean()
    
    st.markdown(f"""
**üí¨ Commentaire:** Parmi les {len(selected_ranking)} FCP s√©lectionn√©s, **{best_fcp['FCP']}** affiche la meilleure 
performance avec **{best_fcp['Performance (%)']:+.2f}%**, tandis que **{worst_fcp['FCP']}** pr√©sente la performance 
la plus faible √† **{worst_fcp['Performance (%)']:+.2f}%**. La performance moyenne du groupe s'√©tablit √† **{avg_perf:+.2f}%**.
""")
    
    st.markdown("---")
    
    # ===================
    # Section 2: Performance Analyses with Tabs
    # ===================
    st.subheader("üìà Analyses de Performance")
    
    tab1, tab2 = st.tabs(["üìÖ Performances Calendaires", "üìä Performances Glissantes"])
    
    with tab1:
        st.markdown("""
        <div class="interpretation-note">
            <strong>üí° Interpr√©tation:</strong> Les performances calendaires mesurent les rendements sur diff√©rentes p√©riodes fixes 
            (semaine, mois, trimestre, ann√©e en cours). Ces m√©triques permettent de comparer la performance r√©cente des FCP.
        </div>
        """, unsafe_allow_html=True)
        
        calendar_data = []
        for fcp in selected_fcps:
            perf = calculate_calendar_performance(full_df, fcp)
            perf['FCP'] = fcp
            calendar_data.append(perf)
        
        calendar_df = pd.DataFrame(calendar_data)
        calendar_df = calendar_df.set_index('FCP')
        
        # Display as table with formatting
        st.markdown("##### üìä Tableau des Performances Calendaires")
        st.dataframe(
            calendar_df.style.background_gradient(cmap='RdYlGn', axis=None)
                            .format("{:+.2f}%"),
            use_container_width=True
        )
    
    with tab2:
        st.markdown("""
        <div class="interpretation-note">
            <strong>üí° Interpr√©tation:</strong> Les performances glissantes mesurent les rendements sur des p√©riodes mobiles 
            (1 mois, 3 mois, 6 mois, 1 an, 5 ans). Elles permettent d'√©valuer la constance des performances dans le temps.
        </div>
        """, unsafe_allow_html=True)
        
        rolling_data = []
        for fcp in selected_fcps:
            perf = calculate_rolling_performance(full_df, fcp)
            perf['FCP'] = fcp
            rolling_data.append(perf)
        
        rolling_df = pd.DataFrame(rolling_data)
        rolling_df = rolling_df.set_index('FCP')
        
        # Display as table with formatting
        st.markdown("##### üìä Tableau des Performances Glissantes")
        st.dataframe(
            rolling_df.style.background_gradient(cmap='RdYlGn', axis=None)
                           .format(lambda x: f"{x:+.2f}%" if pd.notna(x) else "N/A"),
            use_container_width=True
        )
    
    st.markdown("---")
    
    # Section 3: √âvolution des valeurs liquidatives
    st.subheader("üìà √âvolution des Valeurs Liquidatives dans le Temps")
    
    st.markdown("""
    <div class="interpretation-note">
        <strong>üí° Note:</strong> Ce graphique utilise toutes les donn√©es disponibles, ind√©pendamment du filtre de p√©riode s√©lectionn√© dans la barre lat√©rale.
        Vous pouvez choisir la p√©riode et le mode de visualisation ci-dessous.
    </div>
    """, unsafe_allow_html=True)
    
    # Period selection for VL graph
    col1, col2 = st.columns([3, 1])
    
    with col1:
        vl_period = st.radio(
            "S√©lectionnez la p√©riode d'affichage",
            options=['1M', '3M', '6M', '1A', 'Tout'],
            index=4,
            horizontal=True,
            help="Choisissez la p√©riode √† visualiser"
        )
    
    with col2:
        vl_mode = st.radio(
            "Mode",
            options=['Absolue', 'Cumul√© (%)'],
            index=0,
            help="Absolue: valeurs liquidatives r√©elles | Cumul√©: performance en % depuis le d√©but de la p√©riode"
        )
    
    # Filter data based on period selection
    max_date = full_df['Date'].max()
    
    if vl_period == '1M':
        start_date = max_date - timedelta(days=30)
    elif vl_period == '3M':
        start_date = max_date - timedelta(days=90)
    elif vl_period == '6M':
        start_date = max_date - timedelta(days=180)
    elif vl_period == '1A':
        start_date = max_date - timedelta(days=365)
    else:  # 'Tout'
        start_date = full_df['Date'].min()
    
    vl_plot_df = full_df[full_df['Date'] >= start_date].copy()
    
    # Prepare data based on mode
    if vl_mode == 'Cumul√© (%)':
        for fcp in selected_fcps:
            vl_plot_df[fcp] = ((vl_plot_df[fcp] / vl_plot_df[fcp].iloc[0]) - 1) * 100
    
    # Create the evolution chart
    fig_evolution = go.Figure()
    
    for fcp in selected_fcps:
        fig_evolution.add_trace(go.Scatter(
            x=vl_plot_df['Date'],
            y=vl_plot_df[fcp],
            mode='lines',
            name=fcp,
            line=dict(width=2),
            hovertemplate='<b>%{data.name}</b><br>Date: %{x}<br>Valeur: %{y:.2f}<extra></extra>'
        ))
    
    y_title = "Performance Cumul√©e (%)" if vl_mode == 'Cumul√© (%)' else "Valeur Liquidative"
    title_text = f"√âvolution des Valeurs Liquidatives - {vl_period} - {vl_mode}"
    
    fig_evolution.update_layout(
        title=title_text,
        xaxis_title="Date",
        yaxis_title=y_title,
        height=600,
        template="plotly_white",
        hovermode='x unified',
        xaxis=dict(
            rangeslider=dict(visible=True),
            type="date"
        )
    )
    
    st.plotly_chart(fig_evolution, use_container_width=True)
    
    st.markdown("---")
    
    # ===================
    # Section 4: Advanced Analyses with Tabs
    # ===================
    st.subheader("üìä Analyses Avanc√©es")
    
    tab1, tab2, tab3 = st.tabs(["üìà Distributions, Stats & Corr√©lations", "‚ö†Ô∏è Risque", "üéØ Volatilit√©"])
    
    with tab1:
        # Note d'interpr√©tation d√©pliable pour √©conomiser l'espace
        with st.expander("üí° Note de Synth√®se: Analyse des Distributions", expanded=False):
            st.markdown("""
            L'analyse des distributions permet de comprendre le comportement statistique 
            des rendements. Une distribution normale (Skewness proche de 0, Kurtosis proche de 0) indique des variations r√©guli√®res,
            tandis que des valeurs extr√™mes sugg√®rent des comportements atypiques.
            """)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Histogramme des rendements
            fig_hist = go.Figure()
            
            for fcp in selected_fcps:
                returns = filtered_df[fcp].pct_change().dropna() * 100
                fig_hist.add_trace(go.Histogram(
                    x=returns,
                    name=fcp,
                    opacity=0.7,
                    nbinsx=50
                ))
            
            fig_hist.update_layout(
                title="Distribution des Rendements Quotidiens",
                xaxis_title="Rendement (%)",
                yaxis_title="Fr√©quence",
                barmode='overlay',
                height=400,
                template="plotly_white"
            )
            
            st.plotly_chart(fig_hist, use_container_width=True)
        
        with col2:
            # Box plot des rendements
            fig_box = go.Figure()
            
            for fcp in selected_fcps:
                returns = filtered_df[fcp].pct_change().dropna() * 100
                fig_box.add_trace(go.Box(
                    y=returns,
                    name=fcp,
                    boxmean='sd'
                ))
            
            fig_box.update_layout(
                title="Box Plot des Rendements Quotidiens",
                yaxis_title="Rendement (%)",
                height=400,
                template="plotly_white"
            )
            
            st.plotly_chart(fig_box, use_container_width=True)
        
        # Statistiques descriptives
        st.markdown("##### Statistiques Descriptives D√©taill√©es")
        
        stats_data = []
        for fcp in selected_fcps:
            returns = filtered_df[fcp].pct_change().dropna() * 100
            stats_dict = {
                'FCP': fcp,
                'Rendement Moyen (%)': returns.mean(),
                'M√©diane (%)': returns.median(),
                '√âcart-type (%)': returns.std(),
                'Min (%)': returns.min(),
                'Max (%)': returns.max(),
                'Skewness': stats.skew(returns),
                'Kurtosis': stats.kurtosis(returns)
            }
            stats_data.append(stats_dict)
        
        stats_df = pd.DataFrame(stats_data)
        stats_df = stats_df.set_index('FCP')
        
        # Formatage des nombres avec gradient de couleur vert/rouge
        styled_stats = stats_df.style.format("{:.3f}").background_gradient(
            subset=['Rendement Moyen (%)', 'Skewness'], 
            cmap='RdYlGn',  # Rouge pour valeurs n√©gatives, vert pour positives
            vmin=-stats_df['Rendement Moyen (%)'].abs().max(),
            vmax=stats_df['Rendement Moyen (%)'].abs().max()
        )
        st.dataframe(styled_stats, use_container_width=True)
        
        # Quartile analysis
        st.markdown("##### Analyse par Quartiles")
        
        quartile_data = []
        for fcp in selected_fcps:
            returns = filtered_df[fcp].pct_change().dropna() * 100
            q1 = returns.quantile(0.25)
            q2 = returns.quantile(0.50)  # M√©diane
            q3 = returns.quantile(0.75)
            quartile_data.append({
                'FCP': fcp,
                'Q1 (25%)': f"{q1:.3f}%",
                'M√©diane (Q2)': f"{q2:.3f}%",
                'Q3 (75%)': f"{q3:.3f}%",
                'IQR': f"{(q3-q1):.3f}%"
            })
        
        df_quartiles = pd.DataFrame(quartile_data)
        st.dataframe(df_quartiles, use_container_width=True, hide_index=True)
        
        # Note d'interpr√©tation d√©pliable
        with st.expander("üí° Interpr√©tation des Quartiles", expanded=False):
            st.markdown("""
            L'√©cart interquartile (IQR) mesure la dispersion centrale des rendements.
            Un IQR faible indique des rendements plus concentr√©s et donc plus pr√©visibles.
            """)
        
        # ========================================
        # ANALYSE DES CORR√âLATIONS
        # ========================================
        st.markdown("---")
        st.markdown("### üîó Analyse des Corr√©lations")
        
        # Note d'interpr√©tation d√©pliable
        with st.expander("üí° Note: Comprendre les Corr√©lations", expanded=False):
            st.markdown("""
            L'analyse des corr√©lations entre les valeurs liquidatives des diff√©rents FCP 
            permet d'identifier les interd√©pendances et opportunit√©s de diversification. Une faible corr√©lation entre deux FCP 
            indique qu'ils √©voluent de mani√®re relativement ind√©pendante.
            """)
        
        if len(selected_fcps) > 1:
            # Calculate correlation matrix
            correlation_matrix = filtered_df[selected_fcps].corr()
            
            # Heatmap
            fig_corr = go.Figure(data=go.Heatmap(
                z=correlation_matrix.values,
                x=correlation_matrix.columns,
                y=correlation_matrix.index,
                colorscale='RdBu',
                zmid=0,
                text=np.round(correlation_matrix.values, 2),
                texttemplate='%{text}',
                textfont={"size": 10},
                colorbar=dict(title="Corr√©lation")
            ))
            
            fig_corr.update_layout(
                title="Matrice de Corr√©lation des Valeurs Liquidatives",
                height=max(400, len(correlation_matrix) * 30),
                template="plotly_white"
            )
            
            st.plotly_chart(fig_corr, use_container_width=True)
            
            # Find most and least correlated pairs
            corr_pairs = []
            for i in range(len(correlation_matrix.columns)):
                for j in range(i+1, len(correlation_matrix.columns)):
                    corr_pairs.append({
                        'FCP 1': correlation_matrix.columns[i],
                        'FCP 2': correlation_matrix.columns[j],
                        'Corr√©lation': correlation_matrix.iloc[i, j]
                    })
            
            df_corr_pairs = pd.DataFrame(corr_pairs).sort_values('Corr√©lation', ascending=False)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("##### üîó Paires Les Plus Corr√©l√©es")
                top_corr = df_corr_pairs.head(5)
                top_corr['Corr√©lation'] = top_corr['Corr√©lation'].round(3)
                st.dataframe(top_corr, use_container_width=True, hide_index=True)
            
            with col2:
                st.markdown("##### üîÄ Paires Les Moins Corr√©l√©es")
                bottom_corr = df_corr_pairs.tail(5).sort_values('Corr√©lation')
                bottom_corr['Corr√©lation'] = bottom_corr['Corr√©lation'].round(3)
                st.dataframe(bottom_corr, use_container_width=True, hide_index=True)
            
            # Interpr√©tation d√©pliable
            with st.expander("üí° Interpr√©tation des Corr√©lations", expanded=False):
                st.markdown("""
                **Comprendre les corr√©lations:**
                - **Corr√©lation proche de 1:** Les VL √©voluent de mani√®re tr√®s similaire - faible diversification
                - **Corr√©lation proche de 0:** Pas de relation lin√©aire - bonne opportunit√© de diversification
                - **Corr√©lation n√©gative:** Les VL √©voluent de mani√®re oppos√©e - excellente diversification
                """)
        else:
            st.info("S√©lectionnez au moins 2 FCP pour voir l'analyse de corr√©lation.")
    
    with tab2:
        # Note d'interpr√©tation d√©pliable
        with st.expander("üí° Note de Synth√®se: Indicateurs de Risque", expanded=False):
            st.markdown("""
            Les indicateurs de risque mesurent diff√©rents aspects de la volatilit√© 
            et des pertes potentielles. Le Ratio de Sharpe √©value le rendement ajust√© au risque, tandis que VaR et CVaR 
            quantifient les pertes extr√™mes possibles.
            """)
        
        risk_data = []
        for fcp in selected_fcps:
            risk_metrics = calculate_risk_metrics(filtered_df, fcp)
            risk_metrics['FCP'] = fcp
            risk_data.append(risk_metrics)
        
        risk_df = pd.DataFrame(risk_data)
        risk_df = risk_df.set_index('FCP')
        
        # Formatage et affichage avec gradient de couleur
        styled_risk = risk_df.style.format("{:.3f}").background_gradient(
            subset=['Ratio de Sharpe'], 
            cmap='RdYlGn'  # Vert pour valeurs √©lev√©es (bon), rouge pour faibles
        ).background_gradient(
            subset=['Max Drawdown (%)'], 
            cmap='RdYlGn_r'  # Rouge pour valeurs tr√®s n√©gatives (mauvais)
        )
        st.dataframe(styled_risk, use_container_width=True)
        
        # Visualizations
        col1, col2 = st.columns(2)
        
        with col1:
            # Sharpe Ratio comparison
            fig_sharpe = go.Figure()
            fig_sharpe.add_trace(go.Bar(
                x=risk_df.index,
                y=risk_df['Ratio de Sharpe'],
                marker_color='#114B80',
                text=risk_df['Ratio de Sharpe'].round(3).astype(str),
                textposition='outside'
            ))
            
            fig_sharpe.update_layout(
                title="Ratio de Sharpe par FCP",
                xaxis_title="FCP",
                yaxis_title="Ratio de Sharpe",
                height=350,
                template="plotly_white"
            )
            
            st.plotly_chart(fig_sharpe, use_container_width=True)
        
        with col2:
            # Max Drawdown comparison
            fig_dd = go.Figure()
            fig_dd.add_trace(go.Bar(
                x=risk_df.index,
                y=risk_df['Max Drawdown (%)'],
                marker_color='#567389',
                text=risk_df['Max Drawdown (%)'].round(2).astype(str) + '%',
                textposition='outside'
            ))
            
            fig_dd.update_layout(
                title="Drawdown Maximum par FCP",
                xaxis_title="FCP",
                yaxis_title="Max Drawdown (%)",
                height=350,
                template="plotly_white"
            )
            
            st.plotly_chart(fig_dd, use_container_width=True)
        
        # Key insights
        best_sharpe_fcp = risk_df['Ratio de Sharpe'].idxmax()
        worst_dd_fcp = risk_df['Max Drawdown (%)'].idxmin()
        
        st.markdown(f"""
        <div class="insight-box">
            <h4>üéØ Points Cl√©s</h4>
            <p>‚Ä¢ <strong>Meilleur Ratio de Sharpe:</strong> {best_sharpe_fcp} ({risk_df.loc[best_sharpe_fcp, 'Ratio de Sharpe']:.3f})</p>
            <p>‚Ä¢ <strong>Drawdown le plus faible:</strong> {worst_dd_fcp} ({risk_df.loc[worst_dd_fcp, 'Max Drawdown (%)']:.2f}%)</p>
            <p>‚Ä¢ <strong>VaR moyen 95%:</strong> {risk_df['VaR 95% (%)'].mean():.2f}%</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Explications des m√©triques
        with st.expander("‚ÑπÔ∏è Explications des indicateurs de risque"):
            st.markdown("""
            - **Rendement Moyen**: Rendement quotidien moyen en pourcentage
            - **Volatilit√©**: √âcart-type des rendements (mesure de la dispersion)
            - **Ratio de Sharpe**: Rendement ajust√© au risque (plus √©lev√© = meilleur)
            - **VaR 95%**: Value at Risk - perte maximale attendue dans 95% des cas
            - **CVaR 95%**: Conditional VaR - perte moyenne au-del√† du VaR
            - **Skewness**: Asym√©trie de la distribution (< 0 = queue √† gauche)
            - **Kurtosis**: "√âpaisseur" des queues de distribution (> 0 = queues √©paisses)
            - **Max Drawdown**: Perte maximale depuis un sommet historique
            """)
        
        # ========================================
        # ANALYSE AVANC√âE DU RISQUE
        # ========================================
        st.markdown("---")
        st.markdown(f"""
        <div class="insight-box">
            <h4>üéØ Analyse Avanc√©e du Risque</h4>
            <p>Cette section propose une analyse approfondie et dynamique du risque, allant au-del√† des statistiques agr√©g√©es.
            Elle combine une vue temporelle, distributionnelle et comparative pour une compr√©hension compl√®te du profil de risque.</p>
        </div>
        """, unsafe_allow_html=True)
        
        if len(selected_fcps) > 0:
            # S√©lection du FCP principal pour l'analyse d√©taill√©e
            main_fcp = st.selectbox(
                "S√©lectionnez le FCP principal pour l'analyse d√©taill√©e",
                selected_fcps,
                key="advanced_risk_main_fcp"
            )
            
            # ========================================
            # RISK FINGERPRINT - PROFIL DE RISQUE 7D
            # ========================================
            st.markdown("---")
            st.markdown("### üéØ Risk Fingerprint - Profil de Risque Multidimensionnel")
            
            st.markdown("""
            <div class="insight-box">
                <h4>üìä Repr√©sentation du Profil de Risque sur 7 Dimensions</h4>
                <p>Le <strong>Risk Fingerprint</strong> offre une repr√©sentation multidimensionnelle du profil de risque 
                sur 7 dimensions normalis√©es (0-100). Cette visualisation permet d'identifier rapidement les forces et 
                faiblesses du fonds en mati√®re de gestion du risque.</p>
            </div>
            """, unsafe_allow_html=True)
            
            # Calculer le profil 7D pour tous les FCPs s√©lectionn√©s
            profiles_7d = {}
            for fcp in selected_fcps:
                try:
                    profiles_7d[fcp] = calculate_7d_risk_profile(full_df, fcp)
                except Exception as e:
                    st.warning(f"‚ö†Ô∏è Impossible de calculer le profil pour {fcp}: {str(e)}")
            
            if profiles_7d:
                # Normaliser les profils
                normalized_profiles = normalize_7d_risk_profile(profiles_7d)
                
                # Afficher le radar chart pour le FCP principal
                if main_fcp in normalized_profiles:
                    col1, col2 = st.columns([2, 1])
                    
                    with col1:
                        # Cr√©er et afficher le radar chart
                        fig_radar = create_risk_fingerprint_chart(normalized_profiles[main_fcp], main_fcp)
                        st.plotly_chart(fig_radar, use_container_width=True)
                    
                    with col2:
                        st.markdown("##### üìã Scores par Dimension")
                        
                        # Tableau des scores
                        scores_data = []
                        for dimension, score in normalized_profiles[main_fcp].items():
                            scores_data.append({
                                'Dimension': dimension,
                                'Score': f"{score:.1f}/100"
                            })
                        
                        scores_df = pd.DataFrame(scores_data)
                        st.dataframe(scores_df, use_container_width=True, hide_index=True)
                        
                        # Score global
                        global_score = np.mean(list(normalized_profiles[main_fcp].values()))
                        
                        # D√©terminer le niveau de risque
                        if global_score >= 70:
                            risk_level = "Excellent"
                            risk_color = "#28a745"
                        elif global_score >= 50:
                            risk_level = "Bon"
                            risk_color = "#ffc107"
                        else:
                            risk_level = "√Ä Surveiller"
                            risk_color = "#dc3545"
                        
                        st.markdown(f"""
                        <div style="background-color: {risk_color}15; border-left: 4px solid {risk_color}; 
                                    padding: 1rem; border-radius: 5px; margin-top: 1rem;">
                            <div style="font-size: 0.9rem; font-weight: bold; margin-bottom: 0.5rem;">Score Global</div>
                            <div style="font-size: 2rem; font-weight: bold; color: {risk_color};">{global_score:.1f}/100</div>
                            <div style="font-size: 1rem; margin-top: 0.3rem;">{risk_level}</div>
                        </div>
                        """, unsafe_allow_html=True)
                
                # Explication des 7 dimensions
                with st.expander("‚ÑπÔ∏è Explication des 7 Dimensions du Risk Fingerprint"):
                    st.markdown("""
                    Le Risk Fingerprint analyse le profil de risque du fonds sur 7 dimensions cl√©s :
                    
                    1. **Stabilit√©** : Inverse de la volatilit√©. Un score √©lev√© indique des rendements stables et pr√©visibles.
                    
                    2. **R√©silience** : Inverse du drawdown maximum. Un score √©lev√© montre une forte capacit√© √† limiter les pertes en p√©riode adverse.
                    
                    3. **R√©cup√©ration** : Inverse du temps de r√©cup√©ration moyen apr√®s un drawdown. Un score √©lev√© indique une capacit√© rapide √† retrouver les niveaux pr√©c√©dents.
                    
                    4. **Protection Extr√™me** : Inverse de la CVaR (Conditional Value at Risk). Un score √©lev√© signifie une meilleure protection contre les pertes extr√™mes.
                    
                    5. **Asym√©trie** : Skewness normalis√©e. Un score √©lev√© indique une distribution favorable avec plus de gains extr√™mes que de pertes extr√™mes.
                    
                    6. **Sharpe Stable** : Stabilit√© du ratio de Sharpe dans le temps. Un score √©lev√© montre un rendement ajust√© au risque constant et fiable.
                    
                    7. **Pain Ratio** : Rendement ajust√© √† la "douleur" (Ulcer Index). Un score √©lev√© indique que les rendements compensent bien l'inconfort des drawdowns.
                    
                    **Normalisation** : Toutes les dimensions sont normalis√©es sur une √©chelle de 0 √† 100 selon la formule :
                    `Score = (Valeur - Min) / (Max - Min) √ó 100`
                    
                    Cette normalisation permet de comparer les fonds sur une √©chelle commune, ind√©pendamment des unit√©s de mesure d'origine.
                    """)
                
                # Comparaison multi-FCP si plusieurs FCP s√©lectionn√©s
                if len(selected_fcps) > 1:
                    st.markdown("---")
                    st.markdown("##### üìä Comparaison des Profils de Risque")
                    
                    # Cr√©er un tableau comparatif
                    comparison_data = []
                    for fcp_name, profile in normalized_profiles.items():
                        row = {'FCP': fcp_name}
                        row.update({dim: f"{score:.1f}" for dim, score in profile.items()})
                        row['Score Global'] = f"{np.mean(list(profile.values())):.1f}"
                        comparison_data.append(row)
                    
                    comparison_df = pd.DataFrame(comparison_data)
                    st.dataframe(comparison_df, use_container_width=True, hide_index=True)
                    
                    # Identifier les meilleurs et moins bons sur chaque dimension
                    st.markdown("##### üèÜ Forces et Faiblesses par Dimension")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**Top Performers par Dimension**")
                        best_performers = []
                        for dimension in normalized_profiles[main_fcp].keys():
                            best_fcp = max(normalized_profiles.items(), key=lambda x: x[1][dimension])
                            best_performers.append({
                                'Dimension': dimension,
                                'FCP': best_fcp[0],
                                'Score': f"{best_fcp[1][dimension]:.1f}"
                            })
                        
                        best_df = pd.DataFrame(best_performers)
                        st.dataframe(best_df, use_container_width=True, hide_index=True)
                    
                    with col2:
                        st.markdown("**Points d'Attention par Dimension**")
                        worst_performers = []
                        for dimension in normalized_profiles[main_fcp].keys():
                            worst_fcp = min(normalized_profiles.items(), key=lambda x: x[1][dimension])
                            worst_performers.append({
                                'Dimension': dimension,
                                'FCP': worst_fcp[0],
                                'Score': f"{worst_fcp[1][dimension]:.1f}"
                            })
                        
                        worst_df = pd.DataFrame(worst_performers)
                        st.dataframe(worst_df, use_container_width=True, hide_index=True)
            
            # ========================================
            # 1. RISQUE DANS LE TEMPS - DRAWDOWNS
            # ========================================
            st.markdown("---")
            st.markdown("### üìâ 1. Risque dans le Temps : Analyse des Drawdowns")
            
            dd_analysis = analyze_drawdowns(filtered_df, main_fcp)
            
            # M√©triques cl√©s des drawdowns
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric(
                    "Drawdown Maximum",
                    f"{dd_analysis['max_drawdown']:.2f}%",
                    help="Perte maximale depuis un sommet historique"
                )
            
            with col2:
                st.metric(
                    "Drawdown Moyen",
                    f"{dd_analysis['avg_drawdown']:.2f}%",
                    help="Drawdown moyen sur les p√©riodes de baisse"
                )
            
            with col3:
                st.metric(
                    "Ulcer Index",
                    f"{dd_analysis['ulcer_index']:.2f}",
                    help="Mesure de la douleur du risque (racine carr√©e des DD¬≤)"
                )
            
            with col4:
                st.metric(
                    "Pain Ratio",
                    f"{dd_analysis['pain_ratio']:.2f}",
                    help="Rendement total / Ulcer Index (plus √©lev√© = meilleur)"
                )
            
            # Graphique des drawdowns cumul√©s
            fig_dd = go.Figure()
            
            fig_dd.add_trace(go.Scatter(
                x=dd_analysis['dates'],
                y=dd_analysis['drawdown_series'],
                fill='tozeroy',
                fillcolor='rgba(220, 53, 69, 0.3)',
                line=dict(color='#dc3545', width=2),
                name='Drawdown'
            ))
            
            # Marquer les √©pisodes de stress majeurs (DD > 5%)
            major_stress = [ep for ep in dd_analysis['drawdown_episodes'] if ep['depth'] < -5]
            if major_stress:
                for ep in major_stress:
                    fig_dd.add_vrect(
                        x0=ep['start_date'],
                        x1=ep['end_date'],
                        fillcolor="rgba(220, 53, 69, 0.1)",
                        layer="below",
                        line_width=0,
                    )
            
            fig_dd.update_layout(
                title=f"√âvolution des Drawdowns - {main_fcp}",
                xaxis_title="Date",
                yaxis_title="Drawdown (%)",
                height=400,
                template="plotly_white",
                hovermode='x unified'
            )
            
            st.plotly_chart(fig_dd, use_container_width=True)
            
            # Tableau des √©pisodes de stress majeurs
            if major_stress:
                st.markdown("##### üî¥ √âpisodes de Stress Majeurs (Drawdown > 5%)")
                
                stress_data = []
                for ep in sorted(major_stress, key=lambda x: x['depth']):
                    recovery_text = f"{ep['recovery_time']} jours" if ep['recovery_time'] else "Non r√©cup√©r√©"
                    stress_data.append({
                        'D√©but': ep['start_date'].strftime('%Y-%m-%d'),
                        'Fin': ep['end_date'].strftime('%Y-%m-%d'),
                        'Profondeur': f"{ep['depth']:.2f}%",
                        'Dur√©e': f"{ep['duration']} jours",
                        'R√©cup√©ration': recovery_text
                    })
                
                df_stress = pd.DataFrame(stress_data)
                st.dataframe(df_stress, use_container_width=True, hide_index=True)
                
                st.markdown(f"""
                <div class="interpretation-note">
                <strong>üí° Interpr√©tation:</strong> Le fonds a connu <strong>{len(major_stress)} √©pisodes de stress significatifs</strong> 
                (drawdown > 5%). Le drawdown maximum de <strong>{dd_analysis['max_drawdown']:.2f}%</strong> et l'Ulcer Index de 
                <strong>{dd_analysis['ulcer_index']:.2f}</strong> refl√®tent l'intensit√© du risque v√©cu par les investisseurs.
                </div>
                """, unsafe_allow_html=True)
    
    with tab3:
        st.markdown("""
        <div class="insight-box">
            <h4>üéØ Analyse Avanc√©e des R√©gimes de Volatilit√©</h4>
            <p>Cette analyse identifie <strong>3 r√©gimes de volatilit√© distincts</strong> (faible, interm√©diaire, √©lev√©) 
            et √©value la capacit√© du fonds √† cr√©er de la valeur selon les conditions de march√©. Elle permet d'√©valuer 
            la r√©silience en p√©riode de stress et la stabilit√© du profil de risque.</p>
        </div>
        """, unsafe_allow_html=True)
        
        st.markdown("""
        <div class="interpretation-note">
            <strong>üí° Note Importante:</strong> Cette analyse de volatilit√© utilise <strong>toute l'historique disponible</strong>, 
            ind√©pendamment du filtre de p√©riode s√©lectionn√© dans la barre lat√©rale. Cela permet d'avoir une vue compl√®te 
            des r√©gimes de volatilit√© sur toute la dur√©e de vie du fonds.
        </div>
        """, unsafe_allow_html=True)
        
        # S√©lection d'un FCP pour l'analyse
        if len(selected_fcps) > 0:
            fcp_for_analysis = st.selectbox(
                "S√©lectionnez un FCP pour l'analyse des r√©gimes de volatilit√©",
                selected_fcps,
                key="regime_analysis_fcp"
            )
            
            # Analyse des r√©gimes de volatilit√© - UTILISE TOUTE L'HISTORIQUE
            # Use the user-defined volatility window and number of clusters parameters
            regime_analysis = analyze_volatility_regimes(
                full_df, 
                fcp_for_analysis, 
                window=st.session_state.vl_volatility_window,
                n_clusters=st.session_state.vl_n_clusters
            )
            
            st.markdown("---")
            st.markdown("### üìã Synth√®se Ex√©cutive")
            
            current_regime = regime_analysis['current_regime']
            current_regime_name = regime_analysis['current_regime_name']
            regime_stats = regime_analysis['regime_stats']
            
            # Indicateurs de situation actuelle - Enhanced presentation
            regime_icon = {0: "‚úÖ", 1: "‚ö†Ô∏è", 2: "üî¥"}[current_regime]
            regime_color = {0: "#28a745", 1: "#ffc107", 2: "#dc3545"}[current_regime]
            time_in_regime = regime_stats[current_regime]['proportion']
            avg_return_current = regime_stats[current_regime]['avg_return']
            persistence_current = regime_analysis['persistence'][current_regime]
            avg_duration = persistence_current['avg_duration']
            episodes = persistence_current['episodes']
            
            # Beautiful card presentation for executive summary
            st.markdown(f"""
            <div style="background-color: {regime_color}15; border-left: 4px solid {regime_color}; 
                        padding: 1rem; border-radius: 5px; margin-bottom: 1rem;">
                <div style="display: flex; align-items: center; margin-bottom: 0.5rem;">
                    <span style="font-size: 1.5rem; margin-right: 0.5rem;">{regime_icon}</span>
                    <span style="font-size: 1.2rem; font-weight: bold; color: {regime_color};">
                        R√©gime Actuel: {current_regime_name}
                    </span>
                </div>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 0.5rem; margin-top: 0.8rem;">
                    <div>
                        <strong>Volatilit√©:</strong> {regime_stats[current_regime]['avg_volatility']:.2f}%
                    </div>
                    <div>
                        <strong>Proportion:</strong> {time_in_regime:.1f}%
                    </div>
                    <div>
                        <strong>Rendement Moyen:</strong> {avg_return_current:+.3f}%
                    </div>
                    <div>
                        <strong>Dur√©e Moyenne:</strong> {avg_duration:.0f} jours
                    </div>
                    <div>
                        <strong>√âpisodes:</strong> {episodes}
                    </div>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            # Phrases pr√™tes √† l'emploi pour le reporting
            signal_vigilance = ""
            signal_text = ""
            n_clusters = st.session_state.vl_n_clusters
            
            # Determine regime type based on position (0 = lowest volatility, n-1 = highest)
            if current_regime == 0:  # Lowest volatility regime
                signal_text = "CONFORT"
                signal_vigilance = f"""
                <div class="insight-box">
                    <h4>‚úÖ Signal de Confort</h4>
                    <p><strong>Contexte favorable:</strong> Le fonds √©volue actuellement dans un r√©gime de <strong>{current_regime_name}</strong> 
                    ({regime_stats[current_regime]['avg_volatility']:.2f}%), repr√©sentant {regime_stats[current_regime]['proportion']:.1f}% du temps historique.</p>
                    <ul>
                        <li>Le fonds b√©n√©ficie actuellement d'un environnement de march√© stable, avec une <strong>volatilit√© contenue √† {regime_stats[current_regime]['avg_volatility']:.2f}%.</strong></li>
                        <li>Dans ces conditions de faible volatilit√©, le fonds g√©n√®re un <strong>rendement quotidien moyen de {avg_return_current:+.3f}%</strong>, 
                        d√©montrant sa capacit√© √† cr√©er de la valeur en environnement calme.</li>
                        <li>L'analyse historique montre que ce r√©gime de stabilit√© se maintient en moyenne pendant <strong>{avg_duration:.0f} jours ouvr√©s.</strong></li>
                    </ul>
                </div>
                """
            elif current_regime == n_clusters - 1:  # Highest volatility regime
                signal_text = "VIGILANCE √âLEV√âE"
                signal_vigilance = f"""
                <div class="alert-box">
                    <h4>üî¥ Signal de Vigilance √âlev√©e</h4>
                    <p><strong>Contexte de stress:</strong> Le fonds √©volue actuellement dans un r√©gime de <strong>{current_regime_name}</strong> 
                    ({regime_stats[current_regime]['avg_volatility']:.2f}%), situation historiquement observ√©e {regime_stats[current_regime]['proportion']:.1f}% du temps.</p>
                    <ul>
                        <li>Le fonds traverse une p√©riode de <strong>volatilit√© √©lev√©e ({regime_stats[current_regime]['avg_volatility']:.2f}%)</strong>, 
                        n√©cessitant un suivi rapproch√© des positions.</li>
                        <li>En phase de stress, le fonds affiche un <strong>rendement quotidien moyen de {avg_return_current:+.3f}%</strong>, 
                        avec un drawdown maximal observ√© de {regime_stats[current_regime]['max_drawdown']:.2f}%.</li>
                        <li>Historiquement, ces √©pisodes de forte <strong>volatilit√© durent en moyenne {avg_duration:.0f} jours ouvr√©s</strong>, 
                        avec {episodes} occurrences sur la p√©riode analys√©e.</li>
                        <li>La r√©silience du fonds en p√©riode de stress est un facteur cl√© √† surveiller pour √©valuer la qualit√© de gestion du risque.</li>
                    </ul>
                </div>
                """
            else:  # Intermediate regime(s)
                signal_text = "VIGILANCE MOD√âR√âE"
                signal_vigilance = f"""
                <div class="interpretation-note">
                    <h4>‚ö†Ô∏è Signal de Vigilance Mod√©r√©e</h4>
                    <p><strong>Contexte en transition:</strong> Le fonds se trouve dans un r√©gime de <strong>{current_regime_name}</strong> 
                    ({regime_stats[current_regime]['avg_volatility']:.2f}%), phase qui repr√©sente {regime_stats[current_regime]['proportion']:.1f}% du temps historique.</p>
                    <ul>
                        <li>Le fonds traverse actuellement une phase de <strong>volatilit√© mod√©r√©e ({regime_stats[current_regime]['avg_volatility']:.2f}%)</strong>, 
                        caract√©ristique des p√©riodes de transition de march√©.</li>
                        <li>Dans ce r√©gime, le <strong>rendement quotidien moyen s'√©tablit √† {avg_return_current:+.3f}%</strong>, 
                        refl√©tant un √©quilibre risque-rendement ajust√©.</li>
                        <li>La dur√©e moyenne de ce type de p√©riode est de <strong>{avg_duration:.0f} jours</strong>, sugg√©rant une situation temporaire.</li>
                    </ul>
                </div>
                """
            
            st.markdown(signal_vigilance, unsafe_allow_html=True)
            
            # ===============================
            # VISUALISATION DES R√âGIMES
            # ===============================
            st.markdown("---")
            st.markdown("### üìà Cycle de Volatilit√© et Transitions de R√©gimes")
            
            regime_df = regime_analysis['regime_df']
            
            # Graphique temporel des r√©gimes
            fig_regime_timeline = go.Figure()
            
            # Generate color palette dynamically based on number of clusters
            # Green for low volatility -> Yellow -> Orange -> Red for high volatility
            if n_clusters == 2:
                regime_colors_map = {0: '#28a745', 1: '#dc3545'}
            elif n_clusters == 3:
                regime_colors_map = {0: '#28a745', 1: '#ffc107', 2: '#dc3545'}
            elif n_clusters == 4:
                regime_colors_map = {0: '#28a745', 1: '#a8d08d', 2: '#ff8c00', 3: '#dc3545'}
            elif n_clusters == 5:
                regime_colors_map = {0: '#28a745', 1: '#a8d08d', 2: '#ffc107', 3: '#ff8c00', 4: '#dc3545'}
            else:
                # Default: gradient from green to red
                import matplotlib.cm as cm
                import matplotlib.colors as mcolors
                cmap = cm.get_cmap('RdYlGn_r', n_clusters)
                regime_colors_map = {i: mcolors.rgb2hex(cmap(i)) for i in range(n_clusters)}
            
            regime_names = regime_analysis['regime_names']
            
            for regime_id in range(n_clusters):
                regime_data = regime_df[regime_df['Regime'] == regime_id]
                fig_regime_timeline.add_trace(go.Scatter(
                    x=regime_data['Date'],
                    y=regime_data['Volatility'],
                    mode='markers',
                    name=regime_names[regime_id],
                    marker=dict(size=6, color=regime_colors_map[regime_id]),
                    hovertemplate='<b>%{fullData.name}</b><br>Date: %{x}<br>Volatilit√©: %{y:.2f}%<extra></extra>'
                ))
            
            fig_regime_timeline.update_layout(
                title=f"Cycle de Volatilit√© et R√©gimes - {fcp_for_analysis} (Fen√™tre: {st.session_state.vl_volatility_window}j)",
                xaxis_title="Date",
                yaxis_title=f"Volatilit√© Glissante {st.session_state.vl_volatility_window}J (%)",
                height=500,
                template="plotly_white",
                hovermode='closest',
                showlegend=True
            )
            
            st.plotly_chart(fig_regime_timeline, use_container_width=True)
            
            st.markdown("""
            <div class="interpretation-note">
                <strong>üí° Lecture du Graphique:</strong> Ce graphique illustre la dynamique de la volatilit√© dans le temps, 
                avec chaque couleur repr√©sentant un r√©gime distinct. Les transitions entre r√©gimes r√©v√®lent les changements 
                de conditions de march√© et permettent d'anticiper les phases de stress ou de stabilit√©.
            </div>
            """, unsafe_allow_html=True)
            
            # ===============================
            # ANALYSE DESCRIPTIVE PAR R√âGIME
            # ===============================
            st.markdown("---")
            st.markdown("### üìä Analyse Descriptive par R√©gime de Volatilit√©")
            
            # Tableau r√©capitulatif par r√©gime
            regime_summary = []
            for regime_id in range(n_clusters):
                regime_stat = regime_stats[regime_id]
                rr_analysis = regime_analysis['risk_return_analysis'][regime_id]
                persistence = regime_analysis['persistence'][regime_id]
                
                regime_summary.append({
                    'R√©gime': regime_names[regime_id],
                    'Proportion (%)': f"{regime_stat['proportion']:.1f}%",
                    'Volatilit√© Moy. (%)': f"{regime_stat['avg_volatility']:.2f}",
                    'Rendement Moy. (%)': f"{regime_stat['avg_return']:+.3f}",
                    'Max Drawdown (%)': f"{regime_stat['max_drawdown']:.2f}",
                    'Ratio Sharpe': f"{rr_analysis['sharpe_ratio']:.2f}",
                    'Dur√©e Moy. (jours)': f"{persistence['avg_duration']:.0f}",
                    'Nb √âpisodes': persistence['episodes']
                })
            
            regime_summary_df = pd.DataFrame(regime_summary)
            st.dataframe(regime_summary_df, use_container_width=True, hide_index=True)
            
            # Visualisations comparatives
            col1, col2 = st.columns(2)
            
            with col1:
                # Performance moyenne par r√©gime
                fig_perf_regime = go.Figure()
                avg_returns = [regime_stats[i]['avg_return'] for i in range(n_clusters)]
                
                # Use the same color mapping as the timeline chart
                colors_bars = [regime_colors_map[i] for i in range(n_clusters)]
                
                fig_perf_regime.add_trace(go.Bar(
                    x=[regime_names[i] for i in range(n_clusters)],
                    y=avg_returns,
                    marker_color=colors_bars,
                    text=[f"{val:+.3f}%" for val in avg_returns],
                    textposition='outside'
                ))
                
                fig_perf_regime.update_layout(
                    title="Rendement Moyen Quotidien par R√©gime",
                    xaxis_title="R√©gime",
                    yaxis_title="Rendement (%)",
                    height=350,
                    template="plotly_white",
                    showlegend=False
                )
                
                st.plotly_chart(fig_perf_regime, use_container_width=True)
            
            with col2:
                # Proportion du temps par r√©gime
                fig_time_regime = go.Figure()
                proportions = [regime_stats[i]['proportion'] for i in range(n_clusters)]
                
                fig_time_regime.add_trace(go.Pie(
                    labels=[regime_names[i] for i in range(n_clusters)],
                    values=proportions,
                    marker=dict(colors=colors_bars),
                    textinfo='label+percent',
                    hovertemplate='<b>%{label}</b><br>Proportion: %{value:.1f}%<extra></extra>'
                ))
                
                fig_time_regime.update_layout(
                    title="R√©partition du Temps par R√©gime",
                    height=350,
                    template="plotly_white"
                )
                
                st.plotly_chart(fig_time_regime, use_container_width=True)
            
            # ===============================
            # MATRICE DE TRANSITION
            # ===============================
            st.markdown("---")
            st.markdown("### üîÑ Matrice de Transition entre R√©gimes")
            
            transition_probs = regime_analysis['transition_probs']
            
            fig_transition = go.Figure(data=go.Heatmap(
                z=transition_probs * 100,
                x=[regime_names[i] for i in range(n_clusters)],
                y=[regime_names[i] for i in range(n_clusters)],
                colorscale='Blues',
                text=np.round(transition_probs * 100, 1),
                texttemplate='%{text}%',
                textfont={"size": 12},
                colorbar=dict(title="Probabilit√© (%)")
            ))
            
            fig_transition.update_layout(
                title="Probabilit√©s de Transition entre R√©gimes de Volatilit√©",
                xaxis_title="Vers ‚Üí",
                yaxis_title="Depuis ‚Üì",
                height=400,
                template="plotly_white"
            )
            
            st.plotly_chart(fig_transition, use_container_width=True)
            
            # Interpr√©tation des transitions
            max_persistence_regime = max(range(n_clusters), key=lambda i: transition_probs[i, i])
            max_persistence_prob = transition_probs[max_persistence_regime, max_persistence_regime] * 100
            
            st.markdown(f"""
            <div class="interpretation-note">
                <strong>üí° Interpr√©tation de la Matrice:</strong><br>
                ‚Ä¢ La diagonale repr√©sente la <strong>persistance</strong> de chaque r√©gime (probabilit√© de rester dans le m√™me √©tat).<br>
                ‚Ä¢ Le r√©gime <strong>{regime_names[max_persistence_regime]}</strong> pr√©sente la plus forte persistance ({max_persistence_prob:.1f}%), 
                indiquant une tendance √† se maintenir dans cet √©tat.<br>
                ‚Ä¢ Les valeurs hors diagonale indiquent les probabilit√©s de <strong>transition</strong> d'un r√©gime √† un autre, 
                r√©v√©lant la dynamique des cycles de volatilit√©.
            </div>
            """, unsafe_allow_html=True)
            
            # ===============================
            # ANALYSE RISQUE-RENDEMENT
            # ===============================
            st.markdown("---")
            st.markdown("### üíº Analyse Risque-Rendement par R√©gime")
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Ratio de Sharpe par r√©gime
                fig_sharpe_regime = go.Figure()
                sharpe_ratios = [regime_analysis['risk_return_analysis'][i]['sharpe_ratio'] for i in range(n_clusters)]
                
                fig_sharpe_regime.add_trace(go.Bar(
                    x=[regime_names[i] for i in range(n_clusters)],
                    y=sharpe_ratios,
                    marker_color=colors_bars,
                    text=[f"{val:.2f}" for val in sharpe_ratios],
                    textposition='outside'
                ))
                
                fig_sharpe_regime.update_layout(
                    title="Ratio de Sharpe par R√©gime",
                    xaxis_title="R√©gime",
                    yaxis_title="Ratio de Sharpe",
                    height=350,
                    template="plotly_white"
                )
                
                st.plotly_chart(fig_sharpe_regime, use_container_width=True)
            
            with col2:
                # Drawdown maximal par r√©gime
                fig_dd_regime = go.Figure()
                drawdowns = [regime_stats[i]['max_drawdown'] for i in range(n_clusters)]
                
                fig_dd_regime.add_trace(go.Bar(
                    x=[regime_names[i] for i in range(n_clusters)],
                    y=drawdowns,
                    marker_color=colors_bars,
                    text=[f"{val:.2f}%" for val in drawdowns],
                    textposition='outside'
                ))
                
                fig_dd_regime.update_layout(
                    title="Drawdown Maximal par R√©gime",
                    xaxis_title="R√©gime",
                    yaxis_title="Max Drawdown (%)",
                    height=350,
                    template="plotly_white"
                )
                
                st.plotly_chart(fig_dd_regime, use_container_width=True)
            
            # Interpr√©tation risque-rendement - Simplified presentation
            best_sharpe_regime = max(range(n_clusters), key=lambda i: regime_analysis['risk_return_analysis'][i]['sharpe_ratio'])
            worst_dd_regime = min(range(n_clusters), key=lambda i: regime_stats[i]['max_drawdown'])
            
            low_vol_return = regime_stats[0]['avg_return']
            high_vol_return = regime_stats[n_clusters - 1]['avg_return']
            
            value_creation_text = "positive, d√©montrant une bonne capacit√© √† cr√©er de la valeur" if low_vol_return > 0 else "n√©gative, sugg√©rant des difficult√©s √† capitaliser sur la stabilit√©"
            resilience_text = "r√©silient" if high_vol_return > -0.1 else "sous pression"
            
            st.markdown(f"""
**üéØ Interpr√©tation Risque-Rendement**

**Cr√©ation de valeur en p√©riode calme:**  
En r√©gime de {regime_names[0]}, le fonds g√©n√®re un rendement quotidien moyen de **{low_vol_return:+.3f}%**, 
performance {value_creation_text} en environnement stable.

**R√©silience en p√©riode de stress:**  
En r√©gime de {regime_names[n_clusters - 1]}, le rendement moyen est de **{high_vol_return:+.3f}%**, 
indiquant un fonds {resilience_text} face aux turbulences de march√©. Le drawdown maximal de 
**{regime_stats[n_clusters - 1]['max_drawdown']:.2f}%** refl√®te l'exposition au risque extr√™me.

**Profil risque-rendement optimal:**  
Le r√©gime **{regime_names[best_sharpe_regime]}** offre le meilleur ratio de Sharpe 
({regime_analysis['risk_return_analysis'][best_sharpe_regime]['sharpe_ratio']:.2f}), 
indiquant la p√©riode o√π le rendement ajust√© au risque est le plus favorable.
""")
            
            # ===============================
            # STABILIT√â DU PROFIL DE RISQUE
            # ===============================
            st.markdown("---")
            st.markdown("### üé≤ Analyse de Stabilit√© du Profil de Risque")
            
            # Use the highest volatility regime for stability analysis
            high_vol_regime_id = n_clusters - 1
            high_vol_freq = regime_stats[high_vol_regime_id]['proportion']
            high_vol_episodes = regime_analysis['persistence'][high_vol_regime_id]['episodes']
            high_vol_avg_duration = regime_analysis['persistence'][high_vol_regime_id]['avg_duration']
            high_vol_persistence = transition_probs[high_vol_regime_id, high_vol_regime_id] * 100
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric(
                    f"{regime_names[high_vol_regime_id]}",
                    f"{high_vol_freq:.1f}%",
                    help=f"Pourcentage du temps pass√© en r√©gime de {regime_names[high_vol_regime_id]}"
                )
            
            with col2:
                st.metric(
                    "Nombre d'√âpisodes",
                    f"{high_vol_episodes}",
                    help=f"Nombre d'occurrences de {regime_names[high_vol_regime_id]} sur la p√©riode"
                )
            
            with col3:
                st.metric(
                    "Persistance Moyenne",
                    f"{high_vol_avg_duration:.0f} jours",
                    help=f"Dur√©e moyenne d'un √©pisode de {regime_names[high_vol_regime_id]}"
                )
            
            # Score de stabilit√©
            stability_score = 100 - (high_vol_freq + (high_vol_episodes / len(regime_df) * 100 * 10))
            stability_score = max(0, min(100, stability_score))
            
            stability_interpretation = ""
            if stability_score >= 75:
                stability_color = "#28a745"
                stability_level = "Excellent"
                stability_interpretation = f"Le fonds pr√©sente un profil de risque tr√®s stable, avec des √©pisodes de {regime_names[high_vol_regime_id]} rares et de courte dur√©e."
            elif stability_score >= 50:
                stability_color = "#ffc107"
                stability_level = "Bon"
                stability_interpretation = f"Le fonds affiche une stabilit√© correcte, avec une exposition mod√©r√©e aux p√©riodes de {regime_names[high_vol_regime_id]}."
            else:
                stability_color = "#dc3545"
                stability_level = "√Ä Surveiller"
                stability_interpretation = f"Le fonds pr√©sente une exposition significative aux r√©gimes de {regime_names[high_vol_regime_id]}, n√©cessitant une surveillance accrue."
            
            st.markdown(f"""
            <div class="ranking-card">
                <h3>üìä Score de Stabilit√© du Profil de Risque</h3>
                <div style="text-align: center; padding: 0.3rem;">
                    <div style="font-size: 2rem; font-weight: bold;">{stability_score:.0f}/100</div>
                    <div style="font-size: 1rem; margin-top: 0.2rem;">{stability_level}</div>
                </div>
                <div class="ranking-item">
                    <p style="margin: 0;">{stability_interpretation}</p>
                </div>
            </div>
            """, unsafe_allow_html=True)
            
            st.markdown(f"""
            <div class="interpretation-note">
                <strong>üí° √âl√©ments d'Analyse:</strong><br>
                ‚Ä¢ <strong>Fr√©quence:</strong> Le fonds passe {high_vol_freq:.1f}% de son temps en r√©gime de forte volatilit√©, 
                r√©parti sur {high_vol_episodes} √©pisodes distincts.<br>
                ‚Ä¢ <strong>Persistance:</strong> Chaque √©pisode de forte volatilit√© dure en moyenne {high_vol_avg_duration:.0f} jours ouvr√©s, 
                avec une probabilit√© de {high_vol_persistence:.1f}% de se maintenir d'un jour √† l'autre.<br>
                ‚Ä¢ <strong>Implications pour la gestion:</strong> {"Une fr√©quence √©lev√©e et/ou une forte persistance des r√©gimes volatils peuvent indiquer un profil de risque structurellement plus √©lev√©, n√©cessitant une allocation et une gestion active adapt√©es." if high_vol_freq > 25 or high_vol_persistence > 50 else "La faible fr√©quence et persistance des r√©gimes volatils sugg√®rent un profil de risque ma√Ætris√© et coh√©rent avec une strat√©gie de gestion prudente."}
            </div>
            """, unsafe_allow_html=True)
            
            # ===============================
            # AIDE √Ä LA D√âCISION
            # ===============================
            st.markdown("---")
            st.markdown("### üéØ √âl√©ments d'Aide √† la D√©cision")
            
            # Signaux de gestion
            signals = []
            
            # Signal 1: R√©gime actuel
            if current_regime == 0:
                signals.append({
                    'Signal': '‚úÖ Environnement Favorable',
                    'Description': 'R√©gime de faible volatilit√© actuel',
                    'Action Sugg√©r√©e': 'P√©riode propice au renforcement des positions et √† l\'optimisation de l\'allocation'
                })
            elif current_regime == 2:
                signals.append({
                    'Signal': 'üî¥ Vigilance Requise',
                    'Description': 'R√©gime de forte volatilit√© actuel',
                    'Action Sugg√©r√©e': 'Surveiller √©troitement les positions, envisager des couvertures ou r√©ductions d\'exposition'
                })
            else:
                signals.append({
                    'Signal': '‚ö†Ô∏è Phase de Transition',
                    'Description': 'R√©gime de volatilit√© interm√©diaire',
                    'Action Sugg√©r√©e': 'Maintenir une vigilance accrue, anticiper une √©volution vers un r√©gime plus stable ou plus volatile'
                })
            
            # Signal 2: Performance dans le r√©gime actuel
            if current_regime == 0 and low_vol_return > 0.05:
                signals.append({
                    'Signal': '‚úÖ Cr√©ation de Valeur Active',
                    'Description': f'Rendement positif de {low_vol_return:+.3f}% en p√©riode stable',
                    'Action Sugg√©r√©e': 'Profil adapt√© aux investisseurs recherchant une croissance r√©guli√®re'
                })
            elif current_regime == 2 and high_vol_return < -0.2:
                signals.append({
                    'Signal': '‚ö†Ô∏è Sensibilit√© au Stress',
                    'Description': f'Rendement n√©gatif de {high_vol_return:+.3f}% en p√©riode volatile',
                    'Action Sugg√©r√©e': '√âvaluer les m√©canismes de protection et la strat√©gie de gestion du risque'
                })
            
            # Signal 3: Stabilit√©
            if stability_score >= 75:
                signals.append({
                    'Signal': '‚úÖ Profil Stable',
                    'Description': f'Score de stabilit√© √©lev√© ({stability_score:.0f}/100)',
                    'Action Sugg√©r√©e': 'Profil adapt√© aux investisseurs recherchant r√©gularit√© et pr√©visibilit√©'
                })
            elif stability_score < 50:
                signals.append({
                    'Signal': 'üî¥ Volatilit√© Structurelle',
                    'Description': f'Score de stabilit√© faible ({stability_score:.0f}/100)',
                    'Action Sugg√©r√©e': 'Convient aux investisseurs tol√©rants au risque, surveiller la coh√©rence avec le mandat'
                })
            
            signals_df = pd.DataFrame(signals)
            st.dataframe(signals_df, use_container_width=True, hide_index=True)
    
    # Section: Export des Donn√©es
    # ===================
    st.markdown("---")
    st.subheader("üì• Export des Donn√©es et Analyses")
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Export VL data for selected FCPs
        export_df = filtered_df[['Date'] + selected_fcps].copy()
        csv_vl = export_df.to_csv(index=False).encode('utf-8')
        
        st.download_button(
            label="üìä T√©l√©charger Valeurs Liquidatives (CSV)",
            data=csv_vl,
            file_name=f"valeurs_liquidatives_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True,
            help="T√©l√©charger les valeurs liquidatives des FCP s√©lectionn√©s"
        )
    
    with col2:
        # Export summary statistics
        summary_data = []
        for fcp in selected_fcps:
            returns = filtered_df[fcp].pct_change().dropna() * 100
            summary_data.append({
                'FCP': fcp,
                'VL Initiale': filtered_df[fcp].iloc[0],
                'VL Finale': filtered_df[fcp].iloc[-1],
                'Performance (%)': ((filtered_df[fcp].iloc[-1] / filtered_df[fcp].iloc[0]) - 1) * 100,
                'Volatilit√© (%)': returns.std(),
                'Rendement Moyen (%)': returns.mean(),
                'Max': returns.max(),
                'Min': returns.min()
            })
        
        summary_df = pd.DataFrame(summary_data)
        csv_summary = summary_df.to_csv(index=False).encode('utf-8')
        
        st.download_button(
            label="üìà T√©l√©charger Statistiques (CSV)",
            data=csv_summary,
            file_name=f"statistiques_fcps_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv",
            use_container_width=True,
            help="T√©l√©charger les statistiques r√©capitulatives des FCP s√©lectionn√©s"
        )
    
    st.markdown("""
    <div class="interpretation-note">
        <h4>üí° Note sur les Exports</h4>
        <p>Les fichiers export√©s contiennent uniquement les donn√©es des FCP s√©lectionn√©s pour la p√©riode analys√©e. 
        Les statistiques sont calcul√©es sur cette m√™me p√©riode.</p>
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
